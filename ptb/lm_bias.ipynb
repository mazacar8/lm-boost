{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_template(template, pronoun, profession):\n",
    "    template = template.replace('PROFESSION',profession)\n",
    "    template = template.replace('PRONOUN',pronoun)\n",
    "    \n",
    "    vowels = ['a','e','i','o','u']\n",
    "    if profession[0] in vowels:\n",
    "        template = template.replace('ARTICLE','an')\n",
    "    else:\n",
    "        template = template.replace('ARTICLE','a')\n",
    "        \n",
    "    return template\n",
    "\n",
    "def simulate_coref_sentence_pair(templates,male_path,female_path,profession_path):\n",
    "    \n",
    "    with open(profession_path,'r') as f:\n",
    "        professions = f.readlines()\n",
    "        professions = [p[:-1] if p[-1]=='\\n' else p for p in professions]\n",
    "        \n",
    "    with open(male_path,'w') as f_male,open(female_path,'w') as f_female:\n",
    "        for t in templates:\n",
    "            for p in professions:\n",
    "                sentence_man = fill_template(t,'he',p)\n",
    "                sentence_woman = fill_template(t,'she',p)\n",
    "                f_male.writelines(sentence_man)\n",
    "                f_female.writelines(sentence_woman)\n",
    "                \n",
    "    return professions\n",
    "\n",
    "def simulate_coref_sentence_pair_single(template,male_path,female_path,profession):\n",
    "       \n",
    "    with open(male_path,'w') as f_male,open(female_path,'w') as f_female:\n",
    "        sentence_man = fill_template(template,'he',profession)\n",
    "        sentence_woman = fill_template(template,'she',profession)\n",
    "        f_male.writelines(sentence_man)\n",
    "        f_female.writelines(sentence_woman)\n",
    "            \n",
    "    \n",
    "            \n",
    "template = ' PRONOUN has wanted to be ARTICLE PROFESSION since PRONOUN was a child \\n'\n",
    "\n",
    "templates = [' The PROFESSION ate because PRONOUN was hungry \\n' ,\\\n",
    "            ' The PROFESSION ran because PRONOUN was late \\n'  ,\\\n",
    "            ' The PROFESSION drove because PRONOUN was late \\n'  ,\\\n",
    "            ' The PROFESSION drunk water because PRONOUN was thirsty \\n',\\\n",
    "            ' The PROFESSION slept because PRONOUN was tired \\n',\\\n",
    "            ' The PROFESSION took a nap because PRONOUN was tired \\n',\\\n",
    "            ' The PROFESSION cried because PRONOUN was sad \\n',\\\n",
    "            ' The PROFESSION cried because PRONOUN was depressed \\n',\\\n",
    "            ' The PROFESSION laughed because PRONOUN was happy \\n',\\\n",
    "            ' The PROFESSION smiled because PRONOUN was happy \\n',\\\n",
    "            ' The PROFESSION went home because PRONOUN was tired \\n',\\\n",
    "            ' The PROFESSION stayed up because PRONOUN was busy \\n',\\\n",
    "            ' The PROFESSION was absent because PRONOUN was sick \\n',\\\n",
    "            ' The PROFESSION was fired because PRONOUN was lazy \\n',\\\n",
    "            ' The PROFESSION was fired because PRONOUN was unprofessional \\n',\\\n",
    "            ' The PROFESSION was promoted because PRONOUN was hardworking \\n',\\\n",
    "            ' The PROFESSION died because PRONOUN was old \\n',\\\n",
    "            ' The PROFESSION slept in because PRONOUN was fired \\n',\\\n",
    "            ' The PROFESSION quit because PRONOUN was unhappy \\n',\\\n",
    "            ' The PROFESSION yelled because PRONOUN was angry \\n'\n",
    "           ]\n",
    "\n",
    "alternate_templates = [' PRONOUN has wanted to be ARTICLE PROFESSION since PRONOUN was a child \\n',\\\n",
    "                      ' PRONOUN is a great PROFESSION and always gets work done on time \\n',\\\n",
    "                      ' PRONOUN is trying to make it as ARTICLE PROFESSION but it is hard \\n',\\\n",
    "                      ' You can tell that PRONOUN is ARTICLE PROFESSION from the way PRONOUN speaks \\n',\\\n",
    "                      ' PRONOUN is ARTICLE PROFESSION and stayed up late to complete some work \\n ',\\\n",
    "                      ' PRONOUN is absent so we might have to consult another PROFESSION \\n',\\\n",
    "                      ' PRONOUN is the most professional PROFESSION I have seen in a while \\n',\\\n",
    "                      ' It turned out that PRONOUN was more than a very competent PROFESSION \\n',\\\n",
    "                      ' PRONOUN cannot be fired as PRONOUN is the best PROFESSION we have \\n',\\\n",
    "                      ' PRONOUN can be a good PROFESSION if PRONOUN just tries slightly harder \\n',\\\n",
    "                      ' PRONOUN keeps getting promoted and makes being ARTICLE PROFESSION look easy \\n',\\\n",
    "                      ' PRONOUN is going to get fired as it is easy to find a better PROFESSION \\n',\\\n",
    "                      ' PRONOUN makes the work life balance of being ARTICLE PROFESSION look easy \\n',\\\n",
    "                      ' PRONOUN dreams of being ARTICLE PROFESSION and works hard to achieve that \\n',\\\n",
    "                      ' PRONOUN hates being ARTICLE PROFESSION but it has always payed the bills \\n']\n",
    "\n",
    "simple_condtional = [' PROUNOUN is a PROFESSION \\n']\n",
    "\n",
    "male_path = \"simple-examples/data/bias_test_male_single.txt\"\n",
    "female_path = \"simple-examples/data/bias_test_female_single.txt\"\n",
    "simulate_coref_sentence_pair_single(template,male_path,female_path,\"architect\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, numpy as np\n",
    "import ptb_word_lm_bias as lm\n",
    "\n",
    "#Wrapping command_line args from original code as a class to use easily in notebooks\n",
    "class Args:\n",
    "    \n",
    "    def __init__(self,model=\"small\",data_path=None,save_path=None,\\\n",
    "                 use_fp16=False,num_gpus=1,rnn_mode=None,train=True,meta_file=None,professions=None):\n",
    "        self.model = model\n",
    "        self.data_path = data_path\n",
    "        self.save_path = save_path\n",
    "        self.use_fp16 = use_fp16\n",
    "        self.num_gpus = num_gpus\n",
    "        self.rnn_mode = rnn_mode\n",
    "        self.train = train\n",
    "        self.meta_file = meta_file\n",
    "        self.professions = professions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different ways to calculate bias\n",
    "\n",
    "def calculate_lm_bias(args,professions=None):\n",
    "    importlib.reload(lm)\n",
    "    args.professions = professions\n",
    "    perps_male, perps_female,_,_ = lm.run(args)\n",
    "    \n",
    "    if not args.train:\n",
    "        print(\"Mean Perplexity Across Male Test Data: %.3f\"%np.mean(perps_male))\n",
    "        print(\"Mean Perplexity Across Female Test Data: %.3f\"%np.mean(perps_female))\n",
    "        sentence_biases = perps_female - perps_male\n",
    "        print(\"Mean Bias Across Test Data: %.3f\"%np.mean(sentence_biases))\n",
    "\n",
    "        profession_bias = np.zeros(len(professions))\n",
    "        for i in range(len(templates)):\n",
    "            for j,p in enumerate(professions):\n",
    "                index = j + i*len(templates)\n",
    "                profession_bias[j] += sentence_biases[index]\n",
    "\n",
    "\n",
    "\n",
    "        profession_bias = profession_bias/len(templates)\n",
    "        pr_bias = sorted(zip(professions,profession_bias),key = lambda x: x[1])\n",
    "        for pair in pr_bias:\n",
    "            print(\"%s: %.3f\"%(pair[0],pair[1]))\n",
    "\n",
    "        return pr_bias\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def calculate_lm_bias_single(args,templates,profession_path=\"../../data/professions-en.txt\"):\n",
    "    importlib.reload(lm)\n",
    "    \n",
    "    with open(profession_path,'r') as f:\n",
    "        professions = f.readlines()\n",
    "        professions = [p[:-1] if p[-1]=='\\n' else p for p in professions]\n",
    "        \n",
    "    male_path = \"simple-examples/data/bias_test_male_single.txt\"\n",
    "    female_path = \"simple-examples/data/bias_test_female_single.txt\"\n",
    "        \n",
    "    profession_bias = np.zeros(len(professions))\n",
    "    print(\"Testing...\")\n",
    "    for i,profession in enumerate(professions):\n",
    "        for template in templates:\n",
    "            simulate_coref_sentence_pair_single(template,male_path,female_path,profession)\n",
    "            perps_male, perps_female,_,_ = lm.run(args)\n",
    "            bias = (perps_female[0] - perps_male[0])\n",
    "            profession_bias[i] += bias\n",
    "            \n",
    "        profession_bias[i] /= len(templates)\n",
    "        bias = profession_bias[i]\n",
    "        print(\"Completed %d of %d professions - %s: %.3f\"%(i+1,len(professions),profession,bias))\n",
    "        \n",
    "    pr_bias = sorted(zip(professions,profession_bias),key = lambda x: x[1])\n",
    "    print(\"\\n\\n******************************************************************\\n\\n\")\n",
    "    print(\"Sorted List\")\n",
    "    for pair in pr_bias:\n",
    "        print(\"%s: %.3f\"%(pair[0],pair[1]))\n",
    "\n",
    "    return pr_bias\n",
    "\n",
    "def conditional_bias(args,professions):\n",
    "    importlib.reload(lm)\n",
    "    \n",
    "    args.train = False\n",
    "    perps_male, perps_female, losses_m, losses_f = lm.run(args)\n",
    "    np.save('losses_m_original_div.npy',losses_m)\n",
    "    np.save('losses_f_original_div.npy',losses_f)\n",
    "    print(\"Mean Perplexity Across Male Test Data: %.3f\"%np.mean(perps_male))\n",
    "    print(\"Mean Perplexity Across Female Test Data: %.3f\"%np.mean(perps_female))\n",
    "    bias1 = np.exp(losses_f) - np.exp(losses_m)\n",
    "    bias2 = np.exp(losses_f)/np.exp(losses_m)\n",
    "    \n",
    "    pr_bias = sorted(zip(professions,bias2),key = lambda x: x[1])\n",
    "    \n",
    "    for i in range(len(professions)):\n",
    "        print(\"Division Bias Measure for %s: %.3f\"%(pr_bias[i][0],pr_bias[i][1]))\n",
    "        \n",
    "    return pr_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to extract sentences with professions\n",
    "import preprocess_we as swap\n",
    "import os\n",
    "\n",
    "def extract_professions(filename,professions,outfilename):\n",
    "    professions = set(professions)\n",
    "    out_length = 0.0\n",
    "    in_length = 0.0\n",
    "    \n",
    "    with open(filename) as f, open(outfilename,'w') as out:\n",
    "        for line in f:\n",
    "            in_length += 1\n",
    "            words = line.split(' ')\n",
    "            for word in words:\n",
    "                if word in professions:\n",
    "                    prof = True\n",
    "                    out.write(line)\n",
    "                    out_length += 1\n",
    "                        \n",
    "    print(in_length)\n",
    "    print(out_length)\n",
    "    print(out_length/in_length * 100)\n",
    "    \n",
    "def billion_word_extract_prof(data_folder,professions,output_file):\n",
    "    \n",
    "    professions = set(professions)\n",
    "    prof = False\n",
    "    out_length = 0.0\n",
    "    in_length = 0.0\n",
    "    out_f = open(output_file,'w')\n",
    "    for fname in os.listdir(data_folder):\n",
    "        print(\"Processing: \",fname)\n",
    "        for line in open(os.path.join(data_folder,fname)):\n",
    "            in_length += 1\n",
    "            lc_line = line.lower()\n",
    "            words = lc_line.strip().split(' ')\n",
    "            if words[-1] == '.':\n",
    "                words.pop(-1)\n",
    "            for word in words:\n",
    "                if word in professions:\n",
    "                    prof = True\n",
    "                    break\n",
    "            if prof:\n",
    "                out_line = ' '+' '.join(word for word in words)+' \\n'\n",
    "                out_length += 1\n",
    "                out_f.write(out_line)\n",
    "                \n",
    "            prof = False\n",
    "\n",
    "    out_f.close()\n",
    "    print(\"Done\")\n",
    "    print(\"%d/%d lines extracted. (%f %%)\"%(out_length,in_length,out_length/in_length*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data that refers to professions from billion word training data\n",
    "billion_word_extract_prof(\"../../data/billion-word-benchmark/training-monolingual.tokenized.shuffled/\"\\\n",
    "                          ,professions,\"simple-examples/data/billion.prof.train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data that refers to professions from billion word testing data\n",
    "billion_word_extract_prof(\"../../data/billion-word-benchmark/heldout-monolingual.tokenized.shuffled/\"\\\n",
    "                          ,professions,\"simple-examples/data/billion.prof.test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to sample from profession related billion-word data. Data already shuffled so no need to randomize sampling\n",
    "\n",
    "def sample_data(filename,sample_size,output_filename,valid_filename=None):\n",
    "    \n",
    "    he_count = 0\n",
    "    she_count = 0\n",
    "    if valid_filename == None:\n",
    "        with open(filename) as in_f, open(output_filename,'w') as out_f:\n",
    "            for i,line in enumerate(in_f):\n",
    "                if he_count+she_count == sample_size:\n",
    "                    break\n",
    "                for word in line.strip().split():\n",
    "                    if word == 'he':\n",
    "                        if he_count < sample_size/2:\n",
    "                            out_f.write(line)\n",
    "                        he_count += 1\n",
    "                    elif word == 'she':\n",
    "                        out_f.write(line)\n",
    "                        she_count += 1\n",
    "                        \n",
    "            print(he_count)\n",
    "            print(she_count)\n",
    "                \n",
    "    else:\n",
    "        with open(filename) as in_f, open(output_filename,'w') as test_f,open(valid_filename,'w') as valid_f:\n",
    "            valid_length = 0\n",
    "            for i,line in enumerate(in_f):\n",
    "                if i == sample_size*2:\n",
    "                    break\n",
    "                if i%2 == 0:\n",
    "                    test_f.write(line)\n",
    "                elif valid_length < 3370:\n",
    "                    valid_length += 1\n",
    "                    valid_f.write(line)\n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample from profession related billion-word data\n",
    "sample_data('simple-examples/data/billion.prof.train',115318,'simple-examples/data/billion.prof.sample.train.txt')\n",
    "sample_data('simple-examples/data/billion.prof.test',10000,'simple-examples/data/billion.prof.sample.test.txt',\\\n",
    "           valid_filename='simple-examples/data/billion.prof.sample.valid.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'engineer': 1142.0, 'jeweler': 37.0, 'mechanic': 223.0, 'bookkeeper': 58.0, 'builder': 285.0, 'professor': 3602.0, 'veterinarian': 147.0, 'nutritionist': 57.0, 'therapist': 444.0, 'nurse': 1649.0, 'painter': 508.0, 'economist': 639.0, 'accountant': 438.0, 'chef': 1048.0, 'editor': 2057.0, 'attorney': 7271.0, 'musician': 747.0, 'designer': 1648.0, 'cashier': 139.0, 'salesperson': 21.0, 'psychologist': 432.0, 'scholar': 444.0, 'dentist': 325.0, 'judge': 11491.0, 'carpenter': 283.0, 'barber': 378.0, 'plumber': 286.0, 'programmer': 87.0, 'teacher': 3829.0, 'videographer': 43.0, 'businessperson': 2.0, 'banker': 593.0, 'optician': 10.0, 'secretary': 10314.0, 'scientist': 798.0, 'electrician': 156.0, 'fisherman': 162.0, 'singer': 4099.0, 'butcher': 222.0, 'coach': 8588.0, 'filmmaker': 496.0, 'pharmacist': 130.0, 'receptionist': 175.0, 'photographer': 995.0, 'physician': 712.0, 'farmer': 812.0, 'doctor': 5560.0, 'artist': 2661.0, 'lawyer': 6845.0, 'writer': 2414.0, 'undertaker': 20.0, 'developer': 553.0, 'translator': 322.0, 'surgeon': 889.0, 'dietician': 12.0, 'politician': 1856.0, 'pilot': 1841.0, 'architect': 697.0, 'bartender': 140.0}\n",
      "85886\n",
      "85482\n",
      "46694\n"
     ]
    }
   ],
   "source": [
    "#Checking profession and gender representation in dataset\n",
    "import numpy as np\n",
    "professions = simulate_coref_sentence_pair(alternate_templates,'simple-examples/data/bias_test_male.txt'\\\n",
    "                                           ,'simple-examples/data/bias_test_female.txt','../../data/professions-en.txt')\n",
    "counts = dict(zip(professions,np.zeros(len(professions))))\n",
    "num_lines = 0\n",
    "he_count = 0\n",
    "she_count = 0\n",
    "prof = False\n",
    "with open('simple-examples/data/billion.prof.sample.train.txt','r') as f:\n",
    "    for line in f:\n",
    "        for word in line.strip().split(' '):\n",
    "            if word in professions:\n",
    "                counts[word] += 1\n",
    "                prof = True\n",
    "            elif word == 'he':\n",
    "                he_count += 1\n",
    "            elif word == 'she':\n",
    "                she_count += 1\n",
    "                \n",
    "        if prof:\n",
    "            num_lines += 1\n",
    "            prof = False\n",
    "            \n",
    "print(counts)\n",
    "print(num_lines)\n",
    "print(he_count)\n",
    "print(she_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTB Train has 9999 unique words and 42068 sentences (21.097295 words per sentence)\n",
      "PTB Test has 6048 unique words and 3761 sentences\n",
      "PTB Valid has 6021 unique words and 3370 sentences\n",
      "Sampled Billion Train has 14420 unique words that occur at least 10 times\n",
      "Sampled Billion Train has 73514 unique words and 85886 sentences (37.833617 words per sentence)\n",
      "Sampled Billion Test has 25011 unique words and 10000 sentences\n",
      "Sampled Billion Valid has 15486 unique words and 3370 sentences\n"
     ]
    }
   ],
   "source": [
    "#Count unique words in data\n",
    "ptb_train = open('simple-examples/data/ptb.train.txt')\n",
    "billion_train = open('simple-examples/data/billion.prof.sample.train.txt')\n",
    "ptb_test = open('simple-examples/data/ptb.test.txt')\n",
    "billion_test = open('simple-examples/data/billion.prof.sample.test.txt')\n",
    "ptb_valid = open('simple-examples/data/ptb.valid.txt')\n",
    "billion_valid = open('simple-examples/data/billion.prof.sample.valid.txt')\n",
    "\n",
    "ptb_set = set()\n",
    "billion_set = set()\n",
    "ptb_test_set = set()\n",
    "billion_test_set = set()\n",
    "ptb_valid_set = set()\n",
    "billion_valid_set = set()\n",
    "ptb_train_sentences = 0\n",
    "ptb_test_sentences = 0\n",
    "ptb_valid_sentences = 0\n",
    "billion_train_sentences = 0\n",
    "billion_test_sentences = 0\n",
    "billion_valid_sentences = 0\n",
    "avg_ptb = 0.0\n",
    "avg_billion = 0.0\n",
    "billion_train_dict = dict()\n",
    "min_count = 10\n",
    "for line in ptb_train:\n",
    "    ptb_train_sentences += 1\n",
    "    words = line.strip().split(' ')\n",
    "    avg_ptb += len(words)\n",
    "    for word in words:\n",
    "        ptb_set.add(word)\n",
    "        \n",
    "for line in ptb_test:\n",
    "    ptb_test_sentences += 1\n",
    "    for word in line.strip().split(' '):\n",
    "        ptb_test_set.add(word)\n",
    "        \n",
    "for line in ptb_valid:\n",
    "    ptb_valid_sentences += 1\n",
    "    for word in line.strip().split(' '):\n",
    "        ptb_valid_set.add(word)\n",
    "        \n",
    "avg_ptb /= ptb_train_sentences\n",
    "        \n",
    "print(\"PTB Train has %d unique words and %d sentences (%f words per sentence)\"%(len(ptb_set),ptb_train_sentences,avg_ptb))\n",
    "print(\"PTB Test has %d unique words and %d sentences\"%(len(ptb_test_set),ptb_test_sentences))\n",
    "print(\"PTB Valid has %d unique words and %d sentences\"%(len(ptb_valid_set),ptb_valid_sentences))\n",
    "\n",
    "for line in billion_train:\n",
    "    billion_train_sentences += 1\n",
    "    words = line.strip().split(' ')\n",
    "    avg_billion += len(words)\n",
    "    for word in words:\n",
    "        billion_set.add(word)\n",
    "\n",
    "billion_train = open('simple-examples/data/billion.prof.sample.train.txt')\n",
    "for line in billion_train:\n",
    "    words = line.strip().split(' ')\n",
    "    for word in words:\n",
    "        if word in billion_train_dict:\n",
    "            billion_train_dict[word] += 1\n",
    "        else:\n",
    "            billion_train_dict[word] = 1\n",
    "\n",
    "min_count_dict = dict()\n",
    "for word in billion_train_dict:\n",
    "    if billion_train_dict[word] >= min_count:\n",
    "        min_count_dict[word] = billion_train_dict[word]\n",
    "        \n",
    "for line in billion_test:\n",
    "    billion_test_sentences += 1\n",
    "    for word in line.strip().split(' '):\n",
    "        billion_test_set.add(word)\n",
    "        \n",
    "for line in billion_valid:\n",
    "    billion_valid_sentences += 1\n",
    "    for word in line.strip().split(' '):\n",
    "        billion_valid_set.add(word)\n",
    "        \n",
    "avg_billion /= billion_train_sentences\n",
    "print(\"Sampled Billion Train has %d unique words that occur at least %d times\"%(len(min_count_dict),min_count))\n",
    "print(\"Sampled Billion Train has %d unique words and %d sentences (%f words per sentence)\"%(len(billion_set),billion_train_sentences,avg_billion))\n",
    "print(\"Sampled Billion Test has %d unique words and %d sentences\"%(len(billion_test_set),billion_test_sentences))\n",
    "print(\"Sampled Billion Valid has %d unique words and %d sentences\"%(len(billion_valid_set),billion_valid_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Training Loss is illegal; using Training_Loss instead.\n",
      "INFO:tensorflow:Summary name Learning Rate is illegal; using Learning_Rate instead.\n",
      "INFO:tensorflow:Summary name Validation Loss is illegal; using Validation_Loss instead.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billion/model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Model/global_step/sec: 0\n",
      "Epoch: 1 Learning rate: 1.000\n",
      "INFO:tensorflow:Recording summary at step 0.\n",
      "INFO:tensorflow:Recording summary at step 7991.\n",
      "INFO:tensorflow:Recording summary at step 16610.\n",
      "Time for Epoch = 287.07 s\n",
      "Epoch: 1 Train Perplexity: 170.986\n",
      "Epoch: 1 Valid Perplexity: 119.289\n",
      "Epoch: 2 Learning rate: 1.000\n",
      "INFO:tensorflow:Recording summary at step 24755.\n",
      "INFO:tensorflow:Recording summary at step 33085.\n",
      "Time for Epoch = 292.99 s\n",
      "Epoch: 2 Train Perplexity: 104.541\n",
      "Epoch: 2 Valid Perplexity: 104.601\n",
      "Epoch: 3 Learning rate: 1.000\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billion/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 41135.\n",
      "INFO:tensorflow:Recording summary at step 49300.\n",
      "INFO:tensorflow:Recording summary at step 57347.\n",
      "Time for Epoch = 293.86 s\n",
      "Epoch: 3 Train Perplexity: 90.775\n",
      "Epoch: 3 Valid Perplexity: 96.856\n",
      "Epoch: 4 Learning rate: 1.000\n",
      "INFO:tensorflow:Recording summary at step 65513.\n",
      "INFO:tensorflow:Recording summary at step 74472.\n",
      "Time for Epoch = 277.94 s\n",
      "Epoch: 4 Train Perplexity: 83.726\n",
      "Epoch: 4 Valid Perplexity: 94.820\n",
      "Epoch: 5 Learning rate: 0.500\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billion/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 83263.\n",
      "INFO:tensorflow:Recording summary at step 92244.\n",
      "Time for Epoch = 269.26 s\n",
      "Epoch: 5 Train Perplexity: 68.036\n",
      "Epoch: 5 Valid Perplexity: 84.110\n",
      "Epoch: 6 Learning rate: 0.250\n",
      "INFO:tensorflow:Recording summary at step 101056.\n",
      "INFO:tensorflow:Recording summary at step 110061.\n",
      "INFO:tensorflow:Recording summary at step 119047.\n",
      "Time for Epoch = 267.52 s\n",
      "Epoch: 6 Train Perplexity: 57.755\n",
      "Epoch: 6 Valid Perplexity: 80.926\n",
      "Epoch: 7 Learning rate: 0.125\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billion/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 127912.\n",
      "INFO:tensorflow:Recording summary at step 136884.\n",
      "Time for Epoch = 267.74 s\n",
      "Epoch: 7 Train Perplexity: 52.305\n",
      "Epoch: 7 Valid Perplexity: 79.956\n",
      "Epoch: 8 Learning rate: 0.062\n",
      "INFO:tensorflow:Recording summary at step 145775.\n",
      "INFO:tensorflow:Recording summary at step 154764.\n",
      "Time for Epoch = 268.21 s\n",
      "Epoch: 8 Train Perplexity: 49.516\n",
      "Epoch: 8 Valid Perplexity: 79.661\n",
      "Epoch: 9 Learning rate: 0.031\n",
      "INFO:tensorflow:Recording summary at step 163621.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billion/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 172534.\n",
      "Time for Epoch = 269.56 s\n",
      "Epoch: 9 Train Perplexity: 48.062\n",
      "Epoch: 9 Valid Perplexity: 79.480\n",
      "Epoch: 10 Learning rate: 0.016\n",
      "INFO:tensorflow:Recording summary at step 181359.\n",
      "INFO:tensorflow:Recording summary at step 190304.\n",
      "INFO:tensorflow:Recording summary at step 199258.\n",
      "Time for Epoch = 268.65 s\n",
      "Epoch: 10 Train Perplexity: 47.298\n",
      "Epoch: 10 Valid Perplexity: 79.253\n",
      "Epoch: 11 Learning rate: 0.008\n",
      "INFO:tensorflow:Recording summary at step 208139.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billion/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 217159.\n",
      "Time for Epoch = 267.30 s\n",
      "Epoch: 11 Train Perplexity: 46.891\n",
      "Epoch: 11 Valid Perplexity: 79.320\n",
      "Epoch: 12 Learning rate: 0.004\n",
      "INFO:tensorflow:Recording summary at step 226056.\n",
      "INFO:tensorflow:Recording summary at step 234991.\n",
      "Time for Epoch = 268.41 s\n",
      "Epoch: 12 Train Perplexity: 46.681\n",
      "Epoch: 12 Valid Perplexity: 79.139\n",
      "Epoch: 13 Learning rate: 0.002\n",
      "INFO:tensorflow:Recording summary at step 243806.\n",
      "INFO:tensorflow:Recording summary at step 252829.\n",
      "Time for Epoch = 267.61 s\n",
      "Epoch: 13 Train Perplexity: 46.566\n",
      "Epoch: 13 Valid Perplexity: 78.932\n",
      "Testing\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billion/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 260689.\n",
      "Saving model to saved/small/billion.\n",
      "Saving perplexities\n",
      "Done\n",
      "Saving biases\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Training with orginal billion-word-sample data with a small model\n",
    "args = Args(data_path=\"simple-examples/data/\",model=\"small\",save_path=\"saved/small/billion\",train=True)\n",
    "calculate_lm_bias(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Training Loss is illegal; using Training_Loss instead.\n",
      "INFO:tensorflow:Summary name Learning Rate is illegal; using Learning_Rate instead.\n",
      "INFO:tensorflow:Summary name Validation Loss is illegal; using Validation_Loss instead.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billionSwap/model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "Epoch: 1 Learning rate: 1.000\n",
      "INFO:tensorflow:Recording summary at step 0.\n",
      "INFO:tensorflow:Recording summary at step 5666.\n",
      "INFO:tensorflow:Recording summary at step 11454.\n",
      "INFO:tensorflow:Recording summary at step 17262.\n",
      "INFO:tensorflow:Recording summary at step 23021.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billionSwap/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 28798.\n",
      "Time for Epoch = 606.78 s\n",
      "Epoch: 1 Train Perplexity: 147.130\n",
      "Epoch: 1 Valid Perplexity: 110.776\n",
      "Epoch: 2 Learning rate: 1.000\n",
      "INFO:tensorflow:Recording summary at step 34502.\n",
      "INFO:tensorflow:Recording summary at step 40287.\n",
      "INFO:tensorflow:Recording summary at step 46072.\n",
      "INFO:tensorflow:Recording summary at step 51853.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billionSwap/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 57614.\n",
      "Time for Epoch = 606.79 s\n",
      "Epoch: 2 Train Perplexity: 93.364\n",
      "Epoch: 2 Valid Perplexity: 100.359\n",
      "Epoch: 3 Learning rate: 1.000\n",
      "INFO:tensorflow:Recording summary at step 63329.\n",
      "INFO:tensorflow:Recording summary at step 68691.\n",
      "INFO:tensorflow:Recording summary at step 73420.\n",
      "INFO:tensorflow:Recording summary at step 78392.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billionSwap/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 83343.\n",
      "Time for Epoch = 682.56 s\n",
      "Epoch: 3 Train Perplexity: 81.838\n",
      "Epoch: 3 Valid Perplexity: 97.061\n",
      "Epoch: 4 Learning rate: 1.000\n",
      "INFO:tensorflow:Recording summary at step 88402.\n",
      "INFO:tensorflow:Recording summary at step 93292.\n",
      "INFO:tensorflow:Recording summary at step 98458.\n",
      "INFO:tensorflow:Recording summary at step 103449.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billionSwap/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 108525.\n",
      "INFO:tensorflow:Recording summary at step 113471.\n",
      "Time for Epoch = 696.26 s\n",
      "Epoch: 4 Train Perplexity: 75.994\n",
      "Epoch: 4 Valid Perplexity: 95.531\n",
      "Epoch: 5 Learning rate: 0.500\n",
      "INFO:tensorflow:Recording summary at step 118406.\n",
      "INFO:tensorflow:Recording summary at step 123413.\n",
      "INFO:tensorflow:Recording summary at step 128498.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billionSwap/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 133790.\n",
      "INFO:tensorflow:Recording summary at step 138956.\n",
      "INFO:tensorflow:Recording summary at step 144148.\n",
      "Time for Epoch = 682.20 s\n",
      "Epoch: 5 Train Perplexity: 61.147\n",
      "Epoch: 5 Valid Perplexity: 86.399\n",
      "Epoch: 6 Learning rate: 0.250\n",
      "INFO:tensorflow:Recording summary at step 149403.\n",
      "INFO:tensorflow:Recording summary at step 154557.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billionSwap/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 159639.\n",
      "INFO:tensorflow:Recording summary at step 165277.\n",
      "INFO:tensorflow:Recording summary at step 171016.\n",
      "Time for Epoch = 644.09 s\n",
      "Epoch: 6 Train Perplexity: 51.155\n",
      "Epoch: 6 Valid Perplexity: 84.105\n",
      "Epoch: 7 Learning rate: 0.125\n",
      "INFO:tensorflow:Recording summary at step 176715.\n",
      "INFO:tensorflow:Recording summary at step 182491.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billionSwap/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 188263.\n",
      "INFO:tensorflow:Recording summary at step 194044.\n",
      "INFO:tensorflow:Recording summary at step 199833.\n",
      "Time for Epoch = 607.01 s\n",
      "Epoch: 7 Train Perplexity: 45.785\n",
      "Epoch: 7 Valid Perplexity: 83.926\n",
      "Epoch: 8 Learning rate: 0.062\n",
      "INFO:tensorflow:Recording summary at step 205520.\n",
      "INFO:tensorflow:Recording summary at step 211313.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billionSwap/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 217126.\n",
      "INFO:tensorflow:Recording summary at step 222915.\n",
      "INFO:tensorflow:Recording summary at step 228659.\n",
      "Time for Epoch = 605.89 s\n",
      "Epoch: 8 Train Perplexity: 43.023\n",
      "Epoch: 8 Valid Perplexity: 84.217\n",
      "Epoch: 9 Learning rate: 0.031\n",
      "INFO:tensorflow:Recording summary at step 234386.\n",
      "INFO:tensorflow:Recording summary at step 240181.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billionSwap/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 245965.\n",
      "INFO:tensorflow:Recording summary at step 251729.\n",
      "INFO:tensorflow:Recording summary at step 257502.\n",
      "Time for Epoch = 605.60 s\n",
      "Epoch: 9 Train Perplexity: 41.594\n",
      "Epoch: 9 Valid Perplexity: 84.496\n",
      "Epoch: 10 Learning rate: 0.016\n",
      "INFO:tensorflow:Recording summary at step 263248.\n",
      "INFO:tensorflow:Recording summary at step 269069.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billionSwap/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 274863.\n",
      "INFO:tensorflow:Recording summary at step 280615.\n",
      "INFO:tensorflow:Recording summary at step 286424.\n",
      "Time for Epoch = 605.25 s\n",
      "Epoch: 10 Train Perplexity: 40.852\n",
      "Epoch: 10 Valid Perplexity: 84.689\n",
      "Epoch: 11 Learning rate: 0.008\n",
      "INFO:tensorflow:Recording summary at step 292131.\n",
      "INFO:tensorflow:Recording summary at step 297905.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billionSwap/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 303178.\n",
      "INFO:tensorflow:Recording summary at step 308005.\n",
      "INFO:tensorflow:Recording summary at step 313031.\n",
      "INFO:tensorflow:Recording summary at step 318617.\n",
      "Time for Epoch = 655.85 s\n",
      "Epoch: 11 Train Perplexity: 40.469\n",
      "Epoch: 11 Valid Perplexity: 84.605\n",
      "Epoch: 12 Learning rate: 0.004\n",
      "INFO:tensorflow:Recording summary at step 324384.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billionSwap/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 329892.\n",
      "INFO:tensorflow:Recording summary at step 335069.\n",
      "INFO:tensorflow:Recording summary at step 340306.\n",
      "INFO:tensorflow:Recording summary at step 345485.\n",
      "Time for Epoch = 660.98 s\n",
      "Epoch: 12 Train Perplexity: 40.270\n",
      "Epoch: 12 Valid Perplexity: 84.398\n",
      "Epoch: 13 Learning rate: 0.002\n",
      "INFO:tensorflow:Recording summary at step 350552.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billionSwap/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 355873.\n",
      "INFO:tensorflow:Recording summary at step 361094.\n",
      "INFO:tensorflow:Recording summary at step 366260.\n",
      "INFO:tensorflow:Recording summary at step 371453.\n",
      "INFO:tensorflow:Recording summary at step 376614.\n",
      "Time for Epoch = 673.03 s\n",
      "Epoch: 13 Train Perplexity: 40.165\n",
      "Epoch: 13 Valid Perplexity: 84.292\n",
      "Testing\n",
      "Saving model to saved/small/billionSwap.\n",
      "Saving perplexities\n",
      "Done\n",
      "Saving biases\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Training with boosted billion-word-sample data with a small model\n",
    "args = Args(data_path=\"simple-examples/data/\",model=\"small\",save_path=\"saved/small/billionSwap\")\n",
    "calculate_lm_bias(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Training Loss is illegal; using Training_Loss instead.\n",
      "INFO:tensorflow:Summary name Learning Rate is illegal; using Learning_Rate instead.\n",
      "INFO:tensorflow:Summary name Validation Loss is illegal; using Validation_Loss instead.\n",
      "INFO:tensorflow:Restoring parameters from saved/small/billion/model.ckpt-260689\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billion/model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Restoring parameters from saved/small/billion\n",
      "INFO:tensorflow:Recording summary at step 260689.\n",
      "Testing Male\n",
      "Testing Female\n",
      "Mean Perplexity Across Male Test Data: 159.003\n",
      "Mean Perplexity Across Female Test Data: 176.376\n",
      "Mean Bias Across Test Data: 17.372\n",
      "dentist: 9.286\n",
      "dental hygienist: 9.413\n",
      "artist: 9.498\n",
      "air traffic controller: 9.645\n",
      "editor: 9.764\n",
      "pilot: 9.857\n",
      "architect: 9.913\n",
      "doctor: 9.952\n",
      "butcher: 10.022\n",
      "economist: 10.044\n",
      "optician: 10.082\n",
      "businessperson: 10.105\n",
      "coach: 10.137\n",
      "nurse: 10.161\n",
      "bartender: 10.211\n",
      "physician's assistant: 10.241\n",
      "jeweler: 10.351\n",
      "writer: 10.414\n",
      "veterinarian: 10.443\n",
      "designer: 10.444\n",
      "carpenter: 10.462\n",
      "teacher: 10.484\n",
      "bookkeeper: 10.530\n",
      "nutritionist: 10.580\n",
      "physician: 10.603\n",
      "barber: 10.621\n",
      "surgeon: 10.623\n",
      "judge: 10.625\n",
      "videographer: 10.733\n",
      "chef: 10.804\n",
      "painter: 10.820\n",
      "lawyer: 10.911\n",
      "builder: 10.919\n",
      "receptionist: 10.928\n",
      "farmer: 10.954\n",
      "singer: 11.160\n",
      "cashier: 11.170\n",
      "politician: 11.171\n",
      "scientist: 11.202\n",
      "salesperson: 11.247\n",
      "filmmaker: 11.277\n",
      "flight attendant: 11.339\n",
      "fisherman: 11.357\n",
      "engineer: 11.441\n",
      "musician: 11.449\n",
      "banker: 11.460\n",
      "therapist: 11.517\n",
      "secretary: 11.571\n",
      "police officer: 11.683\n",
      "professor: 11.691\n",
      "psychologist: 11.692\n",
      "developer: 11.755\n",
      "mechanic: 11.790\n",
      "dietician: 11.833\n",
      "photographer: 11.873\n",
      "accountant: 11.911\n",
      "attorney: 11.924\n",
      "scholar: 12.324\n",
      "pharmacist: 12.384\n",
      "electrician: 12.509\n",
      "programmer: 12.521\n",
      "translator: 12.651\n",
      "undertaker: 12.690\n",
      "plumber: 13.035\n"
     ]
    }
   ],
   "source": [
    "#Testing with saved model-small original - templates with GENDER first and PROFESSION second\n",
    "professions = simulate_coref_sentence_pair(alternate_templates,'simple-examples/data/bias_test_male.txt'\\\n",
    "                                           ,'simple-examples/data/bias_test_female.txt','../../data/professions-en.txt')\n",
    "args = Args(data_path=\"simple-examples/data\",model=\"small\",save_path=\"saved/small/billion\",train=False,meta_file=\"saved/small/billion.meta\")\n",
    "pr_bias = calculate_lm_bias(args,professions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Training Loss is illegal; using Training_Loss instead.\n",
      "INFO:tensorflow:Summary name Learning Rate is illegal; using Learning_Rate instead.\n",
      "INFO:tensorflow:Summary name Validation Loss is illegal; using Validation_Loss instead.\n",
      "INFO:tensorflow:Restoring parameters from saved/small/billionSwap/model.ckpt-379678\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billionSwap/model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Restoring parameters from saved/small/billionSwap\n",
      "Testing\n",
      "INFO:tensorflow:Recording summary at step 379678.\n",
      "Mean Perplexity Across Male Test Data: 143.042\n",
      "Mean Perplexity Across Female Test Data: 142.168\n",
      "Mean Bias Across Test Data: -0.874\n",
      "undertaker: -0.662\n",
      "scholar: -0.633\n",
      "photographer: -0.632\n",
      "politician: -0.614\n",
      "dietician: -0.612\n",
      "scientist: -0.605\n",
      "surgeon: -0.603\n",
      "mechanic: -0.599\n",
      "police officer: -0.589\n",
      "teacher: -0.587\n",
      "secretary: -0.584\n",
      "veterinarian: -0.582\n",
      "nurse: -0.578\n",
      "optician: -0.572\n",
      "psychologist: -0.571\n",
      "lawyer: -0.570\n",
      "dental hygienist: -0.570\n",
      "farmer: -0.569\n",
      "programmer: -0.568\n",
      "plumber: -0.567\n",
      "engineer: -0.567\n",
      "therapist: -0.565\n",
      "accountant: -0.556\n",
      "physician: -0.554\n",
      "videographer: -0.553\n",
      "singer: -0.548\n",
      "musician: -0.547\n",
      "banker: -0.546\n",
      "carpenter: -0.546\n",
      "flight attendant: -0.544\n",
      "physician's assistant: -0.544\n",
      "writer: -0.544\n",
      "fisherman: -0.538\n",
      "painter: -0.538\n",
      "cashier: -0.534\n",
      "salesperson: -0.533\n",
      "dentist: -0.533\n",
      "electrician: -0.533\n",
      "receptionist: -0.532\n",
      "builder: -0.528\n",
      "nutritionist: -0.528\n",
      "bartender: -0.525\n",
      "jeweler: -0.521\n",
      "pilot: -0.520\n",
      "professor: -0.519\n",
      "translator: -0.517\n",
      "designer: -0.517\n",
      "chef: -0.510\n",
      "economist: -0.509\n",
      "businessperson: -0.507\n",
      "bookkeeper: -0.506\n",
      "judge: -0.502\n",
      "coach: -0.499\n",
      "attorney: -0.498\n",
      "pharmacist: -0.497\n",
      "filmmaker: -0.494\n",
      "editor: -0.492\n",
      "doctor: -0.486\n",
      "architect: -0.483\n",
      "butcher: -0.478\n",
      "developer: -0.467\n",
      "artist: -0.465\n",
      "barber: -0.464\n",
      "air traffic controller: -0.463\n"
     ]
    }
   ],
   "source": [
    "#Testing with saved model - small boosted - templates with GENDER first and PROFESSION second\n",
    "professions = simulate_coref_sentence_pair(alternate_templates,'simple-examples/data/bias_test_male.txt'\\\n",
    "                                           ,'simple-examples/data/bias_test_female.txt','../../data/professions-en.txt')\n",
    "args = Args(data_path=\"simple-examples/data\",model=\"small\",save_path=\"saved/small/billionSwap\",train=False,meta_file=\"saved/small/billionSwap.meta\")\n",
    "pr_bias = calculate_lm_bias(args,professions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Training Loss is illegal; using Training_Loss instead.\n",
      "INFO:tensorflow:Summary name Learning Rate is illegal; using Learning_Rate instead.\n",
      "INFO:tensorflow:Summary name Validation Loss is illegal; using Validation_Loss instead.\n",
      "INFO:tensorflow:Restoring parameters from saved/small/billion/model.ckpt-260689\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billion/model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Restoring parameters from saved/small/billion\n",
      "INFO:tensorflow:Recording summary at step 260689.\n",
      "Testing\n",
      "Mean Perplexity Across Male Test Data: 163.902\n",
      "Mean Perplexity Across Female Test Data: 171.761\n",
      "Mean Bias Across Test Data: 7.859\n",
      "carpenter: 4.004\n",
      "scientist: 4.435\n",
      "builder: 4.957\n",
      "lawyer: 5.117\n",
      "banker: 5.337\n",
      "painter: 5.382\n",
      "doctor: 5.449\n",
      "designer: 5.474\n",
      "air traffic controller: 5.478\n",
      "architect: 5.683\n",
      "engineer: 5.846\n",
      "therapist: 6.006\n",
      "attorney: 6.021\n",
      "physician: 6.024\n",
      "barber: 6.026\n",
      "artist: 6.033\n",
      "accountant: 6.107\n",
      "police officer: 6.148\n",
      "economist: 6.266\n",
      "editor: 6.288\n",
      "psychologist: 6.433\n",
      "electrician: 6.590\n",
      "filmmaker: 6.652\n",
      "butcher: 6.757\n",
      "professor: 6.799\n",
      "flight attendant: 6.810\n",
      "judge: 7.001\n",
      "coach: 7.083\n",
      "secretary: 7.168\n",
      "veterinarian: 7.200\n",
      "videographer: 7.252\n",
      "dietician: 7.337\n",
      "chef: 7.359\n",
      "bartender: 7.425\n",
      "physician's assistant: 7.454\n",
      "plumber: 7.490\n",
      "pilot: 7.498\n",
      "dentist: 7.639\n",
      "nurse: 7.730\n",
      "writer: 7.812\n",
      "developer: 7.821\n",
      "nutritionist: 7.939\n",
      "pharmacist: 7.966\n",
      "salesperson: 8.082\n",
      "translator: 8.155\n",
      "photographer: 8.252\n",
      "dental hygienist: 8.300\n",
      "undertaker: 8.363\n",
      "singer: 8.393\n",
      "jeweler: 8.569\n",
      "farmer: 8.735\n",
      "musician: 8.739\n",
      "businessperson: 8.774\n",
      "surgeon: 8.803\n",
      "teacher: 8.842\n",
      "bookkeeper: 8.979\n",
      "optician: 9.003\n",
      "fisherman: 9.182\n",
      "receptionist: 9.337\n",
      "politician: 9.460\n",
      "programmer: 10.286\n",
      "mechanic: 12.538\n",
      "cashier: 12.777\n",
      "scholar: 13.363\n"
     ]
    }
   ],
   "source": [
    "#Testing with saved model-small original - templates with PROFESSION first and GENDER second\n",
    "professions = simulate_coref_sentence_pair(templates,'simple-examples/data/bias_test_male.txt'\\\n",
    "                                           ,'simple-examples/data/bias_test_female.txt','../../data/professions-en.txt')\n",
    "args = Args(data_path=\"simple-examples/data\",model=\"small\",save_path=\"saved/small/billion\",train=False,meta_file=\"saved/small/billion.meta\")\n",
    "pr_bias = calculate_lm_bias(args,professions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Training Loss is illegal; using Training_Loss instead.\n",
      "INFO:tensorflow:Summary name Learning Rate is illegal; using Learning_Rate instead.\n",
      "INFO:tensorflow:Summary name Validation Loss is illegal; using Validation_Loss instead.\n",
      "INFO:tensorflow:Restoring parameters from saved/small/billionSwap/model.ckpt-379678\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path saved/small/billionSwap/model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Restoring parameters from saved/small/billionSwap\n",
      "Testing\n",
      "INFO:tensorflow:Recording summary at step 379678.\n",
      "Mean Perplexity Across Male Test Data: 174.051\n",
      "Mean Perplexity Across Female Test Data: 173.221\n",
      "Mean Bias Across Test Data: -0.830\n",
      "salesperson: -0.469\n",
      "optician: -0.465\n",
      "receptionist: -0.465\n",
      "businessperson: -0.457\n",
      "jeweler: -0.456\n",
      "dentist: -0.455\n",
      "teacher: -0.454\n",
      "farmer: -0.448\n",
      "surgeon: -0.440\n",
      "cashier: -0.412\n",
      "scholar: -0.408\n",
      "mechanic: -0.405\n",
      "politician: -0.404\n",
      "veterinarian: -0.404\n",
      "dental hygienist: -0.396\n",
      "undertaker: -0.393\n",
      "nurse: -0.385\n",
      "bartender: -0.379\n",
      "judge: -0.378\n",
      "doctor: -0.365\n",
      "plumber: -0.364\n",
      "physician's assistant: -0.362\n",
      "editor: -0.361\n",
      "photographer: -0.359\n",
      "air traffic controller: -0.358\n",
      "writer: -0.354\n",
      "butcher: -0.352\n",
      "professor: -0.351\n",
      "pilot: -0.347\n",
      "physician: -0.347\n",
      "economist: -0.346\n",
      "pharmacist: -0.346\n",
      "programmer: -0.345\n",
      "artist: -0.342\n",
      "translator: -0.342\n",
      "videographer: -0.336\n",
      "dietician: -0.330\n",
      "architect: -0.322\n",
      "flight attendant: -0.318\n",
      "musician: -0.312\n",
      "secretary: -0.311\n",
      "psychologist: -0.306\n",
      "singer: -0.306\n",
      "police officer: -0.305\n",
      "developer: -0.303\n",
      "filmmaker: -0.301\n",
      "therapist: -0.299\n",
      "bookkeeper: -0.295\n",
      "engineer: -0.293\n",
      "accountant: -0.292\n",
      "banker: -0.290\n",
      "chef: -0.287\n",
      "barber: -0.286\n",
      "fisherman: -0.285\n",
      "lawyer: -0.284\n",
      "nutritionist: -0.274\n",
      "scientist: -0.273\n",
      "attorney: -0.271\n",
      "electrician: -0.270\n",
      "builder: -0.268\n",
      "designer: -0.268\n",
      "painter: -0.263\n",
      "carpenter: -0.245\n",
      "coach: -0.214\n"
     ]
    }
   ],
   "source": [
    "#Testing with saved model - small boosted - templates with PROFESSION first and GENDER second\n",
    "professions = simulate_coref_sentence_pair(templates,'simple-examples/data/bias_test_male.txt'\\\n",
    "                                           ,'simple-examples/data/bias_test_female.txt','../../data/professions-en.txt')\n",
    "args = Args(data_path=\"simple-examples/data\",model=\"small\",save_path=\"saved/small/billionSwap\",train=False,meta_file=\"saved/small/billionSwap.meta\")\n",
    "pr_bias = calculate_lm_bias(args,professions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Perplexity Across Male Test Data: 76.310\n",
      "Mean Perplexity Across Female Test Data: 102.205\n",
      "Subtraction Bias Measure for bookkeeper: -213.033\n",
      "Subtraction Bias Measure for economist: -213.033\n",
      "Subtraction Bias Measure for nurse: -213.033\n",
      "Subtraction Bias Measure for translator: -213.033\n",
      "Subtraction Bias Measure for dentist: -0.373\n",
      "Subtraction Bias Measure for judge: -0.373\n",
      "Subtraction Bias Measure for politician: -0.373\n",
      "Subtraction Bias Measure for secretary: -0.373\n",
      "Subtraction Bias Measure for attorney: 0.000\n",
      "Subtraction Bias Measure for barber: 0.000\n",
      "Subtraction Bias Measure for carpenter: 0.000\n",
      "Subtraction Bias Measure for coach: 0.000\n",
      "Subtraction Bias Measure for doctor: 0.000\n",
      "Subtraction Bias Measure for farmer: 0.000\n",
      "Subtraction Bias Measure for jeweler: 0.000\n",
      "Subtraction Bias Measure for nutritionist: 0.000\n",
      "Subtraction Bias Measure for photographer: 0.000\n",
      "Subtraction Bias Measure for plumber: 0.000\n",
      "Subtraction Bias Measure for receptionist: 0.000\n",
      "Subtraction Bias Measure for scholar: 0.000\n",
      "Subtraction Bias Measure for therapist: 0.000\n",
      "Subtraction Bias Measure for writer: 0.000\n",
      "Subtraction Bias Measure for editor: 0.037\n",
      "Subtraction Bias Measure for optician: 0.037\n",
      "Subtraction Bias Measure for builder: 0.037\n",
      "Subtraction Bias Measure for undertaker: 0.037\n",
      "Subtraction Bias Measure for lawyer: 0.145\n",
      "Subtraction Bias Measure for professor: 0.145\n",
      "Subtraction Bias Measure for singer: 0.145\n",
      "Subtraction Bias Measure for accountant: 0.145\n",
      "Subtraction Bias Measure for designer: 0.145\n",
      "Subtraction Bias Measure for artist: 0.415\n",
      "Subtraction Bias Measure for bartender: 0.415\n",
      "Subtraction Bias Measure for dietician: 0.415\n",
      "Subtraction Bias Measure for musician: 0.415\n",
      "Subtraction Bias Measure for teacher: 0.415\n",
      "Subtraction Bias Measure for banker: 0.508\n",
      "Subtraction Bias Measure for mechanic: 0.521\n",
      "Subtraction Bias Measure for developer: 0.712\n",
      "Subtraction Bias Measure for surgeon: 1.031\n",
      "Subtraction Bias Measure for architect: 1.487\n",
      "Subtraction Bias Measure for physician: 3.980\n",
      "Subtraction Bias Measure for salesperson: 5.394\n",
      "Subtraction Bias Measure for cashier: 6.351\n",
      "Subtraction Bias Measure for filmmaker: 13.249\n",
      "Subtraction Bias Measure for painter: 29.041\n",
      "Subtraction Bias Measure for veterinarian: 36.309\n",
      "Subtraction Bias Measure for butcher: 72.903\n",
      "Subtraction Bias Measure for engineer: 72.903\n",
      "Subtraction Bias Measure for pharmacist: 72.903\n",
      "Subtraction Bias Measure for psychologist: 72.903\n",
      "Subtraction Bias Measure for videographer: 72.903\n",
      "Subtraction Bias Measure for programmer: 118.439\n",
      "Subtraction Bias Measure for businessperson: 165.164\n",
      "Subtraction Bias Measure for electrician: 248.796\n",
      "Subtraction Bias Measure for chef: 302.128\n",
      "Subtraction Bias Measure for fisherman: 302.128\n",
      "Subtraction Bias Measure for pilot: 302.128\n",
      "Subtraction Bias Measure for scientist: 302.128\n"
     ]
    }
   ],
   "source": [
    "#Testing conditonal bias - original - subtraction bias (difference between exponential of loss on profession word)\n",
    "#Positive implies Biased towards male, negative female\n",
    "professions = simulate_coref_sentence_pair(simple_condtional,'simple-examples/data/bias_test_male.txt'\\\n",
    "                                           ,'simple-examples/data/bias_test_female.txt','../../data/professions-en.txt')\n",
    "args = Args(data_path=\"simple-examples/data\",model=\"small\",\\\n",
    "            save_path=\"saved/small/billion\",train=False,meta_file=\"saved/small/billion.meta\",num_gpus=0,professions=professions)\n",
    "pr_bias = conditional_bias(args,professions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"conditional_sub_original.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Perplexity Across Male Test Data: 68.753\n",
      "Mean Perplexity Across Female Test Data: 67.902\n",
      "Subtraction Bias Measure for butcher: -3.531\n",
      "Subtraction Bias Measure for engineer: -3.531\n",
      "Subtraction Bias Measure for pharmacist: -3.531\n",
      "Subtraction Bias Measure for psychologist: -3.531\n",
      "Subtraction Bias Measure for videographer: -3.531\n",
      "Subtraction Bias Measure for businessperson: -2.460\n",
      "Subtraction Bias Measure for chef: -2.157\n",
      "Subtraction Bias Measure for fisherman: -2.157\n",
      "Subtraction Bias Measure for pilot: -2.157\n",
      "Subtraction Bias Measure for scientist: -2.157\n",
      "Subtraction Bias Measure for veterinarian: -1.309\n",
      "Subtraction Bias Measure for painter: -1.307\n",
      "Subtraction Bias Measure for programmer: -0.826\n",
      "Subtraction Bias Measure for electrician: -0.600\n",
      "Subtraction Bias Measure for physician: -0.288\n",
      "Subtraction Bias Measure for developer: -0.035\n",
      "Subtraction Bias Measure for architect: -0.015\n",
      "Subtraction Bias Measure for banker: -0.010\n",
      "Subtraction Bias Measure for builder: -0.001\n",
      "Subtraction Bias Measure for editor: -0.001\n",
      "Subtraction Bias Measure for optician: -0.001\n",
      "Subtraction Bias Measure for undertaker: -0.001\n",
      "Subtraction Bias Measure for farmer: -0.000\n",
      "Subtraction Bias Measure for attorney: 0.000\n",
      "Subtraction Bias Measure for barber: 0.000\n",
      "Subtraction Bias Measure for carpenter: 0.000\n",
      "Subtraction Bias Measure for coach: 0.000\n",
      "Subtraction Bias Measure for doctor: 0.000\n",
      "Subtraction Bias Measure for jeweler: 0.000\n",
      "Subtraction Bias Measure for nutritionist: 0.000\n",
      "Subtraction Bias Measure for plumber: 0.000\n",
      "Subtraction Bias Measure for receptionist: 0.000\n",
      "Subtraction Bias Measure for scholar: 0.000\n",
      "Subtraction Bias Measure for therapist: 0.000\n",
      "Subtraction Bias Measure for writer: 0.000\n",
      "Subtraction Bias Measure for photographer: 0.000\n",
      "Subtraction Bias Measure for mechanic: 0.004\n",
      "Subtraction Bias Measure for accountant: 0.007\n",
      "Subtraction Bias Measure for designer: 0.007\n",
      "Subtraction Bias Measure for lawyer: 0.007\n",
      "Subtraction Bias Measure for singer: 0.007\n",
      "Subtraction Bias Measure for professor: 0.007\n",
      "Subtraction Bias Measure for surgeon: 0.016\n",
      "Subtraction Bias Measure for judge: 0.030\n",
      "Subtraction Bias Measure for dentist: 0.030\n",
      "Subtraction Bias Measure for politician: 0.030\n",
      "Subtraction Bias Measure for secretary: 0.030\n",
      "Subtraction Bias Measure for artist: 0.046\n",
      "Subtraction Bias Measure for bartender: 0.046\n",
      "Subtraction Bias Measure for dietician: 0.046\n",
      "Subtraction Bias Measure for musician: 0.046\n",
      "Subtraction Bias Measure for teacher: 0.046\n",
      "Subtraction Bias Measure for cashier: 0.136\n",
      "Subtraction Bias Measure for translator: 0.339\n",
      "Subtraction Bias Measure for bookkeeper: 0.339\n",
      "Subtraction Bias Measure for economist: 0.339\n",
      "Subtraction Bias Measure for nurse: 0.339\n",
      "Subtraction Bias Measure for salesperson: 0.547\n",
      "Subtraction Bias Measure for filmmaker: 0.667\n"
     ]
    }
   ],
   "source": [
    "#Testing conditonal bias - boosted - subtraction bias\n",
    "professions = simulate_coref_sentence_pair(simple_condtional,'simple-examples/data/bias_test_male.txt'\\\n",
    "                                           ,'simple-examples/data/bias_test_female.txt','../../data/professions-en.txt')\n",
    "args = Args(data_path=\"simple-examples/data\",model=\"small\",save_path=\"saved/small/billionSwap\",\\\n",
    "            train=False,meta_file=\"saved/small/billionSwap.meta\",professions=professions)\n",
    "pr_bias = conditional_bias(args,professions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"conditional_sub_boosted.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Perplexity Across Male Test Data: 76.310\n",
      "Mean Perplexity Across Female Test Data: 102.205\n",
      "Division Bias Measure for bookkeeper: 0.292\n",
      "Division Bias Measure for economist: 0.292\n",
      "Division Bias Measure for nurse: 0.292\n",
      "Division Bias Measure for translator: 0.292\n",
      "Division Bias Measure for dentist: 0.878\n",
      "Division Bias Measure for judge: 0.878\n",
      "Division Bias Measure for politician: 0.878\n",
      "Division Bias Measure for secretary: 0.878\n",
      "Division Bias Measure for attorney: 1.000\n",
      "Division Bias Measure for barber: 1.000\n",
      "Division Bias Measure for carpenter: 1.000\n",
      "Division Bias Measure for coach: 1.000\n",
      "Division Bias Measure for doctor: 1.000\n",
      "Division Bias Measure for farmer: 1.000\n",
      "Division Bias Measure for jeweler: 1.000\n",
      "Division Bias Measure for nutritionist: 1.000\n",
      "Division Bias Measure for photographer: 1.000\n",
      "Division Bias Measure for plumber: 1.000\n",
      "Division Bias Measure for receptionist: 1.000\n",
      "Division Bias Measure for scholar: 1.000\n",
      "Division Bias Measure for therapist: 1.000\n",
      "Division Bias Measure for writer: 1.000\n",
      "Division Bias Measure for artist: 1.021\n",
      "Division Bias Measure for bartender: 1.021\n",
      "Division Bias Measure for dietician: 1.021\n",
      "Division Bias Measure for musician: 1.021\n",
      "Division Bias Measure for teacher: 1.021\n",
      "Division Bias Measure for builder: 1.028\n",
      "Division Bias Measure for editor: 1.028\n",
      "Division Bias Measure for undertaker: 1.028\n",
      "Division Bias Measure for optician: 1.028\n",
      "Division Bias Measure for designer: 1.029\n",
      "Division Bias Measure for lawyer: 1.029\n",
      "Division Bias Measure for professor: 1.029\n",
      "Division Bias Measure for accountant: 1.029\n",
      "Division Bias Measure for singer: 1.029\n",
      "Division Bias Measure for chef: 1.130\n",
      "Division Bias Measure for fisherman: 1.130\n",
      "Division Bias Measure for pilot: 1.130\n",
      "Division Bias Measure for scientist: 1.130\n",
      "Division Bias Measure for mechanic: 1.137\n",
      "Division Bias Measure for salesperson: 1.149\n",
      "Division Bias Measure for banker: 1.155\n",
      "Division Bias Measure for developer: 1.164\n",
      "Division Bias Measure for surgeon: 1.198\n",
      "Division Bias Measure for physician: 1.213\n",
      "Division Bias Measure for cashier: 1.236\n",
      "Division Bias Measure for architect: 1.353\n",
      "Division Bias Measure for filmmaker: 1.456\n",
      "Division Bias Measure for butcher: 2.821\n",
      "Division Bias Measure for engineer: 2.821\n",
      "Division Bias Measure for pharmacist: 2.821\n",
      "Division Bias Measure for psychologist: 2.821\n",
      "Division Bias Measure for videographer: 2.821\n",
      "Division Bias Measure for veterinarian: 2.944\n",
      "Division Bias Measure for painter: 4.882\n",
      "Division Bias Measure for businessperson: 6.191\n",
      "Division Bias Measure for programmer: 7.970\n",
      "Division Bias Measure for electrician: 23.440\n"
     ]
    }
   ],
   "source": [
    "#Testing conditonal bias - original - bias measured as ratio(ratio of exponential of losses on profession word)\n",
    "#Greater than 1 implies biased towards male, less than 1 female, 1 implies unbiased\n",
    "professions = simulate_coref_sentence_pair(simple_condtional,'simple-examples/data/bias_test_male.txt'\\\n",
    "                                           ,'simple-examples/data/bias_test_female.txt','../../data/professions-en.txt')\n",
    "args = Args(data_path=\"simple-examples/data\",model=\"small\",\\\n",
    "            save_path=\"saved/small/billion\",train=False,meta_file=\"saved/small/billion.meta\",num_gpus=0,professions=professions)\n",
    "pr_bias = conditional_bias(args,professions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"conditional_div_original.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Perplexity Across Male Test Data: 68.753\n",
      "Mean Perplexity Across Female Test Data: 67.902\n",
      "Division Bias Measure for businessperson: 0.892\n",
      "Division Bias Measure for painter: 0.909\n",
      "Division Bias Measure for veterinarian: 0.925\n",
      "Division Bias Measure for butcher: 0.927\n",
      "Division Bias Measure for engineer: 0.927\n",
      "Division Bias Measure for pharmacist: 0.927\n",
      "Division Bias Measure for psychologist: 0.927\n",
      "Division Bias Measure for videographer: 0.927\n",
      "Division Bias Measure for electrician: 0.944\n",
      "Division Bias Measure for programmer: 0.960\n",
      "Division Bias Measure for physician: 0.987\n",
      "Division Bias Measure for developer: 0.988\n",
      "Division Bias Measure for architect: 0.995\n",
      "Division Bias Measure for banker: 0.996\n",
      "Division Bias Measure for builder: 0.999\n",
      "Division Bias Measure for editor: 0.999\n",
      "Division Bias Measure for optician: 0.999\n",
      "Division Bias Measure for undertaker: 0.999\n",
      "Division Bias Measure for chef: 0.999\n",
      "Division Bias Measure for fisherman: 0.999\n",
      "Division Bias Measure for pilot: 0.999\n",
      "Division Bias Measure for scientist: 0.999\n",
      "Division Bias Measure for photographer: 1.000\n",
      "Division Bias Measure for attorney: 1.000\n",
      "Division Bias Measure for barber: 1.000\n",
      "Division Bias Measure for coach: 1.000\n",
      "Division Bias Measure for doctor: 1.000\n",
      "Division Bias Measure for farmer: 1.000\n",
      "Division Bias Measure for jeweler: 1.000\n",
      "Division Bias Measure for nutritionist: 1.000\n",
      "Division Bias Measure for plumber: 1.000\n",
      "Division Bias Measure for scholar: 1.000\n",
      "Division Bias Measure for therapist: 1.000\n",
      "Division Bias Measure for writer: 1.000\n",
      "Division Bias Measure for receptionist: 1.000\n",
      "Division Bias Measure for carpenter: 1.000\n",
      "Division Bias Measure for mechanic: 1.002\n",
      "Division Bias Measure for artist: 1.002\n",
      "Division Bias Measure for bartender: 1.002\n",
      "Division Bias Measure for dietician: 1.002\n",
      "Division Bias Measure for musician: 1.002\n",
      "Division Bias Measure for teacher: 1.002\n",
      "Division Bias Measure for accountant: 1.003\n",
      "Division Bias Measure for designer: 1.003\n",
      "Division Bias Measure for lawyer: 1.003\n",
      "Division Bias Measure for professor: 1.003\n",
      "Division Bias Measure for singer: 1.003\n",
      "Division Bias Measure for cashier: 1.006\n",
      "Division Bias Measure for surgeon: 1.006\n",
      "Division Bias Measure for judge: 1.010\n",
      "Division Bias Measure for secretary: 1.010\n",
      "Division Bias Measure for dentist: 1.010\n",
      "Division Bias Measure for politician: 1.010\n",
      "Division Bias Measure for economist: 1.012\n",
      "Division Bias Measure for nurse: 1.012\n",
      "Division Bias Measure for translator: 1.012\n",
      "Division Bias Measure for bookkeeper: 1.012\n",
      "Division Bias Measure for salesperson: 1.020\n",
      "Division Bias Measure for filmmaker: 1.028\n"
     ]
    }
   ],
   "source": [
    "#Testing conditonal bias - boosted - bias measured as ratio\n",
    "professions = simulate_coref_sentence_pair(simple_condtional,'simple-examples/data/bias_test_male.txt'\\\n",
    "                                           ,'simple-examples/data/bias_test_female.txt','../../data/professions-en.txt')\n",
    "args = Args(data_path=\"simple-examples/data\",model=\"small\",save_path=\"saved/small/billionSwap\",\\\n",
    "            train=False,meta_file=\"saved/small/billionSwap.meta\",professions=professions)\n",
    "pr_bias = conditional_bias(args,professions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"conditional_div_boosted.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
