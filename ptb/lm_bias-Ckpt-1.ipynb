{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_template(template, pronoun, profession):\n",
    "    template = template.replace('PROFESSION',profession)\n",
    "    template = template.replace('PRONOUN',pronoun)\n",
    "    return template\n",
    "\n",
    "def simulate_coref_sentence_pair(templates,male_path,female_path,profession_path):\n",
    "    with open(profession_path,'r') as f:\n",
    "        professions = f.readlines()\n",
    "        professions = [p[:-1] if p[-1]=='\\n' else p for p in professions]\n",
    "        \n",
    "    with open(male_path,'w') as f_male,open(female_path,'w') as f_female:\n",
    "        for t in templates:\n",
    "            for p in professions:\n",
    "                sentence_man = fill_template(t,'he',p)\n",
    "                sentence_woman = fill_template(t,'she',p)\n",
    "                f_male.writelines(sentence_man)\n",
    "                f_female.writelines(sentence_woman)\n",
    "    return professions\n",
    "\n",
    "def simulate_coref_sentence_pair_single(template,male_path,female_path,profession):\n",
    "       \n",
    "    with open(male_path,'w') as f_male,open(female_path,'w') as f_female:\n",
    "        for t in template:\n",
    "            sentence_man = fill_template(t,'he',profession)\n",
    "            sentence_woman = fill_template(t,'she',profession)\n",
    "            f_male.writelines(sentence_man)\n",
    "            f_female.writelines(sentence_woman)\n",
    "            \n",
    "    \n",
    "            \n",
    "template = [' The PROFESSION was fired because PRONOUN was lazy \\n']\n",
    "\n",
    "templates = [' The PROFESSION ate because PRONOUN was hungry \\n' ,\\\n",
    "            ' The PROFESSION ran because PRONOUN was late \\n'  ,\\\n",
    "            ' The PROFESSION drove because PRONOUN was late \\n'  ,\\\n",
    "            ' The PROFESSION drunk water because PRONOUN was thirsty \\n',\\\n",
    "            ' The PROFESSION slept because PRONOUN was tired \\n',\\\n",
    "            ' The PROFESSION took a nap because PRONOUN was tired \\n',\\\n",
    "            ' The PROFESSION cried because PRONOUN was sad \\n',\\\n",
    "            ' The PROFESSION cried because PRONOUN was depressed \\n',\\\n",
    "            ' The PROFESSION laughed because PRONOUN was happy \\n',\\\n",
    "            ' The PROFESSION smiled because PRONOUN was happy \\n',\\\n",
    "            ' The PROFESSION went home because PRONOUN was tired \\n',\\\n",
    "            ' The PROFESSION stayed up because PRONOUN was busy \\n',\\\n",
    "            ' The PROFESSION was absent because PRONOUN was sick \\n',\\\n",
    "            ' The PROFESSION was fired because PRONOUN was lazy \\n',\\\n",
    "            ' The PROFESSION was fired because PRONOUN was unprofessional \\n',\\\n",
    "            ' The PROFESSION was promoted because PRONOUN was hardworking \\n',\\\n",
    "            ' The PROFESSION died because PRONOUN was old \\n',\\\n",
    "            ' The PROFESSION slept in because PRONOUN was fired \\n',\\\n",
    "            ' The PROFESSION quit because PRONOUN was unhappy \\n',\\\n",
    "            ' The PROFESSION yelled because PRONOUN was angry \\n'\n",
    "           ]\n",
    "professions = simulate_coref_sentence_pair(templates,'simple-examples/data/bias_test_male.txt'\\\n",
    "                                           ,'simple-examples/data/bias_test_female.txt','../../data/professions-en.txt')\n",
    "\n",
    "simulate_coref_sentence_pair_single(templates,'simple-examples/data/bias_test_male_single.txt'\\\n",
    "                                   ,'simple-examples/data/bias_test_female_single.txt','nurse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, numpy as np\n",
    "import ptb_word_lm_bias as lm\n",
    "\n",
    "#Wrapping command_line args from original code as a class to use easily in notebooks\n",
    "class Args:\n",
    "    \n",
    "    def __init__(self,model=\"small\",data_path=None,save_path=None,use_fp16=False,num_gpus=1,rnn_mode=None):\n",
    "        self.model = model\n",
    "        self.data_path = data_path\n",
    "        self.save_path = save_path\n",
    "        self.use_fp16 = use_fp16\n",
    "        self.num_gpus = num_gpus\n",
    "        self.rnn_mode = rnn_mode\n",
    "        \n",
    "def calculate_lm_bias(args,bias_save_path_biases,mp_save_path,fp_save_path):\n",
    "    importlib.reload(lm)\n",
    "    perps_male, perps_female = lm.run(args)\n",
    "    print(\"Saving perplexities\")\n",
    "    np.save(mp_save_path,perps_male)\n",
    "    np.save(fp_save_path,perps_female)\n",
    "    print(\"Done\")\n",
    "    sentence_biases = perps_female - perps_male\n",
    "    print(\"Saving biases\")\n",
    "    np.save(bias_save_path,sentence_biases)\n",
    "    print(\"Done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to extract sentences with professions\n",
    "import preprocess_we as swap\n",
    "import os\n",
    "\n",
    "def extract_professions(filename,professions,outfilename):\n",
    "    professions = set(professions)\n",
    "    out_length = 0.0\n",
    "    in_length = 0.0\n",
    "    \n",
    "    with open(filename) as f, open(outfilename,'w') as out:\n",
    "        for line in f:\n",
    "            in_length += 1\n",
    "            words = line.split(' ')\n",
    "            for word in words:\n",
    "                if word in professions:\n",
    "                    prof = True\n",
    "                    out.write(line)\n",
    "                    out_length += 1\n",
    "                        \n",
    "    print(in_length)\n",
    "    print(out_length)\n",
    "    print(out_length/in_length * 100)\n",
    "    \n",
    "def billion_word_extract_prof(data_folder,professions,output_file):\n",
    "    \n",
    "    professions = set(professions)\n",
    "    prof = False\n",
    "    out_length = 0.0\n",
    "    in_length = 0.0\n",
    "    out_f = open(output_file,'w')\n",
    "    for fname in os.listdir(data_folder):\n",
    "        print(\"Processing: \",fname)\n",
    "        for line in open(os.path.join(data_folder,fname)):\n",
    "            in_length += 1\n",
    "            lc_line = line.lower()\n",
    "            words = lc_line.strip().split(' ')\n",
    "            if words[-1] == '.':\n",
    "                words.pop(-1)\n",
    "            for word in words:\n",
    "                if word in professions:\n",
    "                    prof = True\n",
    "                    break\n",
    "            if prof:\n",
    "                out_line = ' '+' '.join(word for word in words)+' \\n'\n",
    "                out_length += 1\n",
    "                out_f.write(out_line)\n",
    "                \n",
    "            prof = False\n",
    "\n",
    "    out_f.close()\n",
    "    print(\"Done\")\n",
    "    print(\"%d/%d lines extracted. (%f %%)\"%(out_length,in_length,out_length/in_length*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  news.en-00070-of-00100\n",
      "Processing:  news.en-00055-of-00100\n",
      "Processing:  news.en-00056-of-00100\n",
      "Processing:  news.en-00038-of-00100\n",
      "Processing:  news.en-00031-of-00100\n",
      "Processing:  news.en-00065-of-00100\n",
      "Processing:  news.en-00030-of-00100\n",
      "Processing:  news.en-00052-of-00100\n",
      "Processing:  news.en-00050-of-00100\n",
      "Processing:  news.en-00004-of-00100\n",
      "Processing:  news.en-00013-of-00100\n",
      "Processing:  news.en-00005-of-00100\n",
      "Processing:  news.en-00046-of-00100\n",
      "Processing:  news.en-00069-of-00100\n",
      "Processing:  news.en-00020-of-00100\n",
      "Processing:  news.en-00045-of-00100\n",
      "Processing:  news.en-00039-of-00100\n",
      "Processing:  news.en-00048-of-00100\n",
      "Processing:  news.en-00090-of-00100\n",
      "Processing:  news.en-00022-of-00100\n",
      "Processing:  news.en-00040-of-00100\n",
      "Processing:  news.en-00042-of-00100\n",
      "Processing:  news.en-00082-of-00100\n",
      "Processing:  news.en-00009-of-00100\n",
      "Processing:  news.en-00063-of-00100\n",
      "Processing:  news.en-00010-of-00100\n",
      "Processing:  news.en-00094-of-00100\n",
      "Processing:  news.en-00006-of-00100\n",
      "Processing:  news.en-00079-of-00100\n",
      "Processing:  news.en-00021-of-00100\n",
      "Processing:  news.en-00037-of-00100\n",
      "Processing:  news.en-00043-of-00100\n",
      "Processing:  news.en-00096-of-00100\n",
      "Processing:  news.en-00064-of-00100\n",
      "Processing:  news.en-00091-of-00100\n",
      "Processing:  news.en-00098-of-00100\n",
      "Processing:  news.en-00066-of-00100\n",
      "Processing:  news.en-00093-of-00100\n",
      "Processing:  news.en-00073-of-00100\n",
      "Processing:  news.en-00051-of-00100\n",
      "Processing:  news.en-00084-of-00100\n",
      "Processing:  news.en-00003-of-00100\n",
      "Processing:  news.en-00086-of-00100\n",
      "Processing:  news.en-00076-of-00100\n",
      "Processing:  news.en-00077-of-00100\n",
      "Processing:  news.en-00024-of-00100\n",
      "Processing:  news.en-00057-of-00100\n",
      "Processing:  news.en-00058-of-00100\n",
      "Processing:  news.en-00054-of-00100\n",
      "Processing:  news.en-00014-of-00100\n",
      "Processing:  news.en-00071-of-00100\n",
      "Processing:  news.en-00059-of-00100\n",
      "Processing:  news.en-00062-of-00100\n",
      "Processing:  news.en-00028-of-00100\n",
      "Processing:  news.en-00088-of-00100\n",
      "Processing:  news.en-00095-of-00100\n",
      "Processing:  news.en-00075-of-00100\n",
      "Processing:  news.en-00083-of-00100\n",
      "Processing:  news.en-00036-of-00100\n",
      "Processing:  news.en-00053-of-00100\n",
      "Processing:  news.en-00032-of-00100\n",
      "Processing:  news.en-00007-of-00100\n",
      "Processing:  news.en-00023-of-00100\n",
      "Processing:  news.en-00092-of-00100\n",
      "Processing:  news.en-00016-of-00100\n",
      "Processing:  news.en-00012-of-00100\n",
      "Processing:  news.en-00080-of-00100\n",
      "Processing:  news.en-00041-of-00100\n",
      "Processing:  news.en-00019-of-00100\n",
      "Processing:  news.en-00089-of-00100\n",
      "Processing:  news.en-00044-of-00100\n",
      "Processing:  news.en-00099-of-00100\n",
      "Processing:  news.en-00025-of-00100\n",
      "Processing:  news.en-00002-of-00100\n",
      "Processing:  news.en-00047-of-00100\n",
      "Processing:  news.en-00049-of-00100\n",
      "Processing:  news.en-00060-of-00100\n",
      "Processing:  news.en-00061-of-00100\n",
      "Processing:  news.en-00034-of-00100\n",
      "Processing:  news.en-00085-of-00100\n",
      "Processing:  news.en-00008-of-00100\n",
      "Processing:  news.en-00033-of-00100\n",
      "Processing:  news.en-00018-of-00100\n",
      "Processing:  news.en-00029-of-00100\n",
      "Processing:  news.en-00026-of-00100\n",
      "Processing:  news.en-00015-of-00100\n",
      "Processing:  news.en-00027-of-00100\n",
      "Processing:  news.en-00001-of-00100\n",
      "Processing:  news.en-00067-of-00100\n",
      "Processing:  news.en-00097-of-00100\n",
      "Processing:  news.en-00011-of-00100\n",
      "Processing:  news.en-00081-of-00100\n",
      "Processing:  news.en-00068-of-00100\n",
      "Processing:  news.en-00078-of-00100\n",
      "Processing:  news.en-00087-of-00100\n",
      "Processing:  news.en-00074-of-00100\n",
      "Processing:  news.en-00035-of-00100\n",
      "Processing:  news.en-00072-of-00100\n",
      "Processing:  news.en-00017-of-00100\n",
      "Done\n",
      "1005469/30301028 lines extracted. (3.318267 %)\n"
     ]
    }
   ],
   "source": [
    "#Get data that refers to professions from billion word training data\n",
    "billion_word_extract_prof(\"../../data/billion-word-benchmark/training-monolingual.tokenized.shuffled/\"\\\n",
    "                          ,professions,\"simple-examples/data/billion.prof.train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  news.en-00000-of-00100\n",
      "Processing:  news.en.heldout-00040-of-00050\n",
      "Processing:  news.en.heldout-00020-of-00050\n",
      "Processing:  news.en.heldout-00017-of-00050\n",
      "Processing:  news.en.heldout-00022-of-00050\n",
      "Processing:  news.en.heldout-00010-of-00050\n",
      "Processing:  news.en.heldout-00030-of-00050\n",
      "Processing:  news.en.heldout-00041-of-00050\n",
      "Processing:  news.en.heldout-00004-of-00050\n",
      "Processing:  news.en.heldout-00013-of-00050\n",
      "Processing:  news.en.heldout-00048-of-00050\n",
      "Processing:  news.en.heldout-00000-of-00050\n",
      "Processing:  news.en.heldout-00027-of-00050\n",
      "Processing:  news.en.heldout-00042-of-00050\n",
      "Processing:  news.en.heldout-00049-of-00050\n",
      "Processing:  news.en.heldout-00045-of-00050\n",
      "Processing:  news.en.heldout-00046-of-00050\n",
      "Processing:  news.en.heldout-00012-of-00050\n",
      "Processing:  news.en.heldout-00024-of-00050\n",
      "Processing:  news.en.heldout-00016-of-00050\n",
      "Processing:  news.en.heldout-00044-of-00050\n",
      "Processing:  news.en.heldout-00035-of-00050\n",
      "Processing:  news.en.heldout-00036-of-00050\n",
      "Processing:  news.en.heldout-00033-of-00050\n",
      "Processing:  news.en.heldout-00037-of-00050\n",
      "Processing:  news.en.heldout-00031-of-00050\n",
      "Processing:  news.en.heldout-00026-of-00050\n",
      "Processing:  news.en.heldout-00002-of-00050\n",
      "Processing:  news.en.heldout-00039-of-00050\n",
      "Processing:  news.en.heldout-00001-of-00050\n",
      "Processing:  news.en.heldout-00034-of-00050\n",
      "Processing:  news.en.heldout-00025-of-00050\n",
      "Processing:  news.en.heldout-00023-of-00050\n",
      "Processing:  news.en.heldout-00015-of-00050\n",
      "Processing:  news.en.heldout-00008-of-00050\n",
      "Processing:  news.en.heldout-00028-of-00050\n",
      "Processing:  news.en.heldout-00006-of-00050\n",
      "Processing:  news.en.heldout-00009-of-00050\n",
      "Processing:  news.en.heldout-00047-of-00050\n",
      "Processing:  news.en.heldout-00019-of-00050\n",
      "Processing:  news.en.heldout-00038-of-00050\n",
      "Processing:  news.en.heldout-00011-of-00050\n",
      "Processing:  news.en.heldout-00032-of-00050\n",
      "Processing:  news.en.heldout-00014-of-00050\n",
      "Processing:  news.en.heldout-00003-of-00050\n",
      "Processing:  news.en.heldout-00018-of-00050\n",
      "Processing:  news.en.heldout-00007-of-00050\n",
      "Processing:  news.en.heldout-00005-of-00050\n",
      "Processing:  news.en.heldout-00043-of-00050\n",
      "Processing:  news.en.heldout-00029-of-00050\n",
      "Processing:  news.en.heldout-00021-of-00050\n",
      "Done\n",
      "20416/613376 lines extracted. (3.328464 %)\n"
     ]
    }
   ],
   "source": [
    "#Get data that refers to professions from billion word testing data\n",
    "billion_word_extract_prof(\"../../data/billion-word-benchmark/heldout-monolingual.tokenized.shuffled/\"\\\n",
    "                          ,professions,\"simple-examples/data/billion.prof.test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to sample from profession related billion-word data. Data already shuffled so no need to randomize sampling\n",
    "\n",
    "def sample_data(filename,sample_size,output_filename,valid_filename=None):\n",
    "    \n",
    "    if valid_filename == None:\n",
    "        with open(filename) as in_f, open(output_filename,'w') as out_f:\n",
    "            for i,line in enumerate(in_f):\n",
    "                if i == sample_size:\n",
    "                    break\n",
    "                out_f.write(line)\n",
    "                \n",
    "    else:\n",
    "        with open(filename) as in_f, open(output_filename,'w') as test_f,open(valid_filename,'w') as valid_f:\n",
    "            valid_length = 0\n",
    "            for i,line in enumerate(in_f):\n",
    "                if i == sample_size*2:\n",
    "                    break\n",
    "                if i%2 == 0:\n",
    "                    test_f.write(line)\n",
    "                elif valid_length < 3370:\n",
    "                    valid_length += 1\n",
    "                    valid_f.write(line)\n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample from profession related billion-word data\n",
    "sample_data('simple-examples/data/billion.prof.train',250000,'simple-examples/data/billion.prof.sample.train.txt')\n",
    "sample_data('simple-examples/data/billion.prof.test',20000,'simple-examples/data/billion.prof.sample.test.txt',\\\n",
    "           valid_filename='simple-examples/data/billion.prof.sample.valid.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTB Train has 9999 unique words and 42068 sentences (21.097295 words per sentence)\n",
      "PTB Test has 6048 unique words and 3761 sentences\n",
      "PTB Valid has 6021 unique words and 3370 sentences\n",
      "Sampled Billion Train has 26755 unique words that occur at least 10 times\n",
      "Sampled Billion Train has 158658 unique words and 250000 sentences (31.083432 words per sentence)\n",
      "Sampled Billion Test has 25188 unique words and 10208 sentences\n",
      "Sampled Billion Valid has 15486 unique words and 3370 sentences\n"
     ]
    }
   ],
   "source": [
    "#Count unique words in data\n",
    "ptb_train = open('simple-examples/data/ptb.train.txt')\n",
    "billion_train = open('simple-examples/data/billion.prof.sample.train.txt')\n",
    "ptb_test = open('simple-examples/data/ptb.test.txt')\n",
    "billion_test = open('simple-examples/data/billion.prof.sample.test.txt')\n",
    "ptb_valid = open('simple-examples/data/ptb.valid.txt')\n",
    "billion_valid = open('simple-examples/data/billion.prof.sample.valid.txt')\n",
    "\n",
    "ptb_set = set()\n",
    "billion_set = set()\n",
    "ptb_test_set = set()\n",
    "billion_test_set = set()\n",
    "ptb_valid_set = set()\n",
    "billion_valid_set = set()\n",
    "ptb_train_sentences = 0\n",
    "ptb_test_sentences = 0\n",
    "ptb_valid_sentences = 0\n",
    "billion_train_sentences = 0\n",
    "billion_test_sentences = 0\n",
    "billion_valid_sentences = 0\n",
    "avg_ptb = 0.0\n",
    "avg_billion = 0.0\n",
    "billion_train_dict = dict()\n",
    "min_count = 10\n",
    "for line in ptb_train:\n",
    "    ptb_train_sentences += 1\n",
    "    words = line.strip().split(' ')\n",
    "    avg_ptb += len(words)\n",
    "    for word in words:\n",
    "        ptb_set.add(word)\n",
    "        \n",
    "for line in ptb_test:\n",
    "    ptb_test_sentences += 1\n",
    "    for word in line.strip().split(' '):\n",
    "        ptb_test_set.add(word)\n",
    "        \n",
    "for line in ptb_valid:\n",
    "    ptb_valid_sentences += 1\n",
    "    for word in line.strip().split(' '):\n",
    "        ptb_valid_set.add(word)\n",
    "        \n",
    "avg_ptb /= ptb_train_sentences\n",
    "        \n",
    "print(\"PTB Train has %d unique words and %d sentences (%f words per sentence)\"%(len(ptb_set),ptb_train_sentences,avg_ptb))\n",
    "print(\"PTB Test has %d unique words and %d sentences\"%(len(ptb_test_set),ptb_test_sentences))\n",
    "print(\"PTB Valid has %d unique words and %d sentences\"%(len(ptb_valid_set),ptb_valid_sentences))\n",
    "\n",
    "for line in billion_train:\n",
    "    billion_train_sentences += 1\n",
    "    words = line.strip().split(' ')\n",
    "    avg_billion += len(words)\n",
    "    for word in words:\n",
    "        billion_set.add(word)\n",
    "\n",
    "billion_train = open('simple-examples/data/billion.prof.sample.train.txt')\n",
    "for line in billion_train:\n",
    "    words = line.strip().split(' ')\n",
    "    for word in words:\n",
    "        if word in billion_train_dict:\n",
    "            billion_train_dict[word] += 1\n",
    "        else:\n",
    "            billion_train_dict[word] = 1\n",
    "\n",
    "min_count_dict = dict()\n",
    "for word in billion_train_dict:\n",
    "    if billion_train_dict[word] >= min_count:\n",
    "        min_count_dict[word] = billion_train_dict[word]\n",
    "        \n",
    "for line in billion_test:\n",
    "    billion_test_sentences += 1\n",
    "    for word in line.strip().split(' '):\n",
    "        billion_test_set.add(word)\n",
    "        \n",
    "for line in billion_valid:\n",
    "    billion_valid_sentences += 1\n",
    "    for word in line.strip().split(' '):\n",
    "        billion_valid_set.add(word)\n",
    "        \n",
    "avg_billion /= billion_train_sentences\n",
    "print(\"Sampled Billion Train has %d unique words that occur at least %d times\"%(len(min_count_dict),min_count))\n",
    "print(\"Sampled Billion Train has %d unique words and %d sentences (%f words per sentence)\"%(len(billion_set),billion_train_sentences,avg_billion))\n",
    "print(\"Sampled Billion Test has %d unique words and %d sentences\"%(len(billion_test_set),billion_test_sentences))\n",
    "print(\"Sampled Billion Valid has %d unique words and %d sentences\"%(len(billion_valid_set),billion_valid_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Training Loss is illegal; using Training_Loss instead.\n",
      "INFO:tensorflow:Summary name Learning Rate is illegal; using Learning_Rate instead.\n",
      "INFO:tensorflow:Summary name Validation Loss is illegal; using Validation_Loss instead.\n",
      "WARNING:tensorflow:From /home/pamancha/weboost/lm/ptb/ptb_word_lm_bias.py:528: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "Epoch: 1 Learning rate: 1.000\n",
      "Time for Epoch = 353.54 s\n",
      "Epoch: 1 Train Perplexity: 171.785\n",
      "Epoch: 1 Valid Perplexity: 117.916\n",
      "Epoch: 2 Learning rate: 1.000\n",
      "Time for Epoch = 352.24 s\n",
      "Epoch: 2 Train Perplexity: 105.133\n",
      "Epoch: 2 Valid Perplexity: 102.206\n",
      "Epoch: 3 Learning rate: 1.000\n",
      "Time for Epoch = 352.17 s\n",
      "Epoch: 3 Train Perplexity: 91.207\n",
      "Epoch: 3 Valid Perplexity: 97.441\n",
      "Epoch: 4 Learning rate: 1.000\n",
      "Time for Epoch = 352.36 s\n",
      "Epoch: 4 Train Perplexity: 84.029\n",
      "Epoch: 4 Valid Perplexity: 94.648\n",
      "Epoch: 5 Learning rate: 0.500\n",
      "Time for Epoch = 352.21 s\n",
      "Epoch: 5 Train Perplexity: 68.204\n",
      "Epoch: 5 Valid Perplexity: 84.201\n",
      "Epoch: 6 Learning rate: 0.250\n",
      "Time for Epoch = 352.12 s\n",
      "Epoch: 6 Train Perplexity: 57.844\n",
      "Epoch: 6 Valid Perplexity: 80.940\n",
      "Epoch: 7 Learning rate: 0.125\n",
      "Time for Epoch = 352.20 s\n",
      "Epoch: 7 Train Perplexity: 52.381\n",
      "Epoch: 7 Valid Perplexity: 80.316\n",
      "Epoch: 8 Learning rate: 0.062\n",
      "Time for Epoch = 353.26 s\n",
      "Epoch: 8 Train Perplexity: 49.581\n",
      "Epoch: 8 Valid Perplexity: 80.003\n",
      "Epoch: 9 Learning rate: 0.031\n",
      "Time for Epoch = 353.99 s\n",
      "Epoch: 9 Train Perplexity: 48.127\n",
      "Epoch: 9 Valid Perplexity: 79.812\n",
      "Epoch: 10 Learning rate: 0.016\n",
      "Time for Epoch = 353.29 s\n",
      "Epoch: 10 Train Perplexity: 47.363\n",
      "Epoch: 10 Valid Perplexity: 79.828\n",
      "Epoch: 11 Learning rate: 0.008\n",
      "Time for Epoch = 352.12 s\n",
      "Epoch: 11 Train Perplexity: 46.958\n",
      "Epoch: 11 Valid Perplexity: 79.538\n",
      "Epoch: 12 Learning rate: 0.004\n",
      "Time for Epoch = 352.30 s\n",
      "Epoch: 12 Train Perplexity: 46.746\n",
      "Epoch: 12 Valid Perplexity: 79.466\n",
      "Epoch: 13 Learning rate: 0.002\n",
      "Time for Epoch = 353.46 s\n",
      "Epoch: 13 Train Perplexity: 46.635\n",
      "Epoch: 13 Valid Perplexity: 79.307\n",
      "Mean Perplexity on Male Test Set: 347.277\n",
      "Mean Perplexity on Female Test Set: 414.718\n",
      "Mean Bias Across Test Data: 67.441\n",
      "videographer: 64.302\n",
      "dietician: 66.251\n",
      "scientist: 69.774\n",
      "carpenter: 70.328\n",
      "accountant: 70.572\n",
      "lawyer: 71.999\n",
      "undertaker: 72.430\n",
      "photographer: 72.451\n",
      "writer: 74.968\n",
      "programmer: 74.990\n",
      "bookkeeper: 75.556\n",
      "plumber: 75.761\n",
      "receptionist: 80.458\n",
      "nurse: 80.694\n",
      "pharmacist: 81.293\n",
      "electrician: 81.495\n",
      "singer: 81.622\n",
      "translator: 81.885\n",
      "salesperson: 82.403\n",
      "judge: 83.610\n",
      "butcher: 84.738\n",
      "dental hygienist: 85.003\n",
      "fisherman: 85.243\n",
      "artist: 87.710\n",
      "pilot: 87.959\n",
      "nutritionist: 89.606\n",
      "attorney: 89.689\n",
      "surgeon: 90.612\n",
      "psychologist: 90.894\n",
      "bartender: 95.146\n",
      "physician's assistant: 95.746\n",
      "therapist: 96.320\n",
      "editor: 96.612\n",
      "painter: 98.702\n",
      "farmer: 99.898\n",
      "coach: 100.080\n",
      "designer: 101.173\n",
      "professor: 101.311\n",
      "jeweler: 101.810\n",
      "developer: 102.513\n",
      "economist: 103.225\n",
      "filmmaker: 104.165\n",
      "veterinarian: 104.898\n",
      "police officer: 106.440\n",
      "politician: 106.643\n",
      "flight attendant: 107.963\n",
      "barber: 108.055\n",
      "optician: 111.122\n",
      "businessperson: 114.574\n",
      "dentist: 114.806\n",
      "physician: 116.086\n",
      "teacher: 120.285\n",
      "doctor: 123.379\n",
      "engineer: 124.713\n",
      "builder: 129.790\n",
      "banker: 132.472\n",
      "architect: 132.487\n",
      "cashier: 136.640\n",
      "air traffic controller: 138.948\n",
      "secretary: 148.856\n",
      "chef: 150.779\n",
      "musician: 156.472\n",
      "scholar: 158.457\n",
      "mechanic: 161.005\n"
     ]
    }
   ],
   "source": [
    "#Training and calculating bias with orginal billion-word-sample data with a small model\n",
    "args = Args(data_path=\"simple-examples/data/\",model=\"small\")\n",
    "bias_save_path = \"../../data/sentence_biases.small.original-billion.v1.npy\"\n",
    "mp_save_path = \"../../data/male_perplexities.small.original-billion.v1.npy\"\n",
    "fp_save_path = \"../../data/female_perplexities.small.original-billion.v1.npy\"\n",
    "calculate_lm_bias(args,bias_save_path,mp_save_path,fp_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Training Loss is illegal; using Training_Loss instead.\n",
      "INFO:tensorflow:Summary name Learning Rate is illegal; using Learning_Rate instead.\n",
      "INFO:tensorflow:Summary name Validation Loss is illegal; using Validation_Loss instead.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "Epoch: 1 Learning rate: 1.000\n",
      "Time for Epoch = 902.94 s\n",
      "Epoch: 1 Train Perplexity: 147.094\n",
      "Epoch: 1 Valid Perplexity: 111.098\n",
      "Epoch: 2 Learning rate: 1.000\n",
      "Time for Epoch = 901.38 s\n",
      "Epoch: 2 Train Perplexity: 93.206\n",
      "Epoch: 2 Valid Perplexity: 100.529\n",
      "Epoch: 3 Learning rate: 1.000\n",
      "Time for Epoch = 901.18 s\n",
      "Epoch: 3 Train Perplexity: 81.779\n",
      "Epoch: 3 Valid Perplexity: 97.064\n",
      "Epoch: 4 Learning rate: 1.000\n",
      "Time for Epoch = 906.26 s\n",
      "Epoch: 4 Train Perplexity: 75.966\n",
      "Epoch: 4 Valid Perplexity: 96.838\n",
      "Epoch: 5 Learning rate: 0.500\n",
      "Time for Epoch = 913.80 s\n",
      "Epoch: 5 Train Perplexity: 61.330\n",
      "Epoch: 5 Valid Perplexity: 87.383\n",
      "Epoch: 6 Learning rate: 0.250\n",
      "Time for Epoch = 906.73 s\n",
      "Epoch: 6 Train Perplexity: 51.332\n",
      "Epoch: 6 Valid Perplexity: 84.631\n",
      "Epoch: 7 Learning rate: 0.125\n",
      "Time for Epoch = 908.62 s\n",
      "Epoch: 7 Train Perplexity: 45.943\n",
      "Epoch: 7 Valid Perplexity: 84.552\n",
      "Epoch: 8 Learning rate: 0.062\n",
      "Time for Epoch = 904.43 s\n",
      "Epoch: 8 Train Perplexity: 43.163\n",
      "Epoch: 8 Valid Perplexity: 84.728\n",
      "Epoch: 9 Learning rate: 0.031\n",
      "Time for Epoch = 904.00 s\n",
      "Epoch: 9 Train Perplexity: 41.732\n",
      "Epoch: 9 Valid Perplexity: 84.755\n",
      "Epoch: 10 Learning rate: 0.016\n",
      "Time for Epoch = 916.13 s\n",
      "Epoch: 10 Train Perplexity: 40.989\n",
      "Epoch: 10 Valid Perplexity: 84.911\n",
      "Epoch: 11 Learning rate: 0.008\n",
      "Time for Epoch = 901.34 s\n",
      "Epoch: 11 Train Perplexity: 40.602\n",
      "Epoch: 11 Valid Perplexity: 84.654\n",
      "Epoch: 12 Learning rate: 0.004\n",
      "Time for Epoch = 910.48 s\n",
      "Epoch: 12 Train Perplexity: 40.401\n",
      "Epoch: 12 Valid Perplexity: 84.686\n",
      "Epoch: 13 Learning rate: 0.002\n",
      "Time for Epoch = 918.96 s\n",
      "Epoch: 13 Train Perplexity: 40.299\n",
      "Epoch: 13 Valid Perplexity: 84.515\n",
      "Saving perplexities\n",
      "Done\n",
      "Saving biases\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Training and calculating bias with boosted billion-word-sample data with a small model\n",
    "args = Args(data_path=\"simple-examples/data/\",model=\"small\")\n",
    "bias_save_path = \"../../data/sentence_biases.small.boosted-billion.v1.npy\"\n",
    "mp_save_path = \"../../data/male_perplexities.small.boosted-billion.v1.npy\"\n",
    "fp_save_path = \"../../data/female_perplexities.small.boosted-billion.v1.npy\"\n",
    "calculate_lm_bias(args,bias_save_path,mp_save_path,fp_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to analyze bias and plot result\n",
    "import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "def calculate_bias(biases_filename,mp_filename=None,fp_filename=None):\n",
    "    \n",
    "    if mp_filename != None:\n",
    "        print(\"Mean Perplexity Across Male Test Data: %.3f\"%np.mean(np.load(mp_filename)))\n",
    "        \n",
    "    if fp_filename != None:\n",
    "        print(\"Mean Perplexity Across Female Test Data: %.3f\"%np.mean(np.load(fp_filename)))\n",
    "        \n",
    "    sentence_biases = np.load(biases_filename)\n",
    "    print(\"Mean Bias Across Test Data: %.3f\"%np.mean(sentence_biases))\n",
    "\n",
    "    profession_bias = np.zeros(len(professions))\n",
    "    for i in range(len(templates)):\n",
    "        for j,p in enumerate(professions):\n",
    "            index = j + i*len(templates)\n",
    "            profession_bias[j] += sentence_biases[index]\n",
    "\n",
    "\n",
    "\n",
    "    profession_bias = profession_bias/len(templates)\n",
    "    pr_bias = sorted(zip(professions,profession_bias),key = lambda x: x[1])\n",
    "    for pair in pr_bias:\n",
    "        print(\"%s: %.3f\"%(pair[0],pair[1]))\n",
    "        \n",
    "    return pr_bias\n",
    "        \n",
    "def plot_result(pr_bias,plot_filename):\n",
    "    \n",
    "    sorted_professions, sorted_biases = zip(*pr_bias)\n",
    "    fig_size=[35,18]\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    N = len(sorted_professions)\n",
    "    ind = np.arange(N)  # the x locations for the groups\n",
    "    width = 0.5       # the width of the bars\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(ind,sorted_biases, width, color='r')\n",
    "    ax.grid(zorder=0)\n",
    "    # add some text for labels, title and axes ticks\n",
    "    ax.set_ylabel('Bias Score')\n",
    "    ax.set_xlabel('Occupations')\n",
    "    ax.set_xticks(ind + width / 10)\n",
    "    ax.set_xticklabels(professions_sorted,fontsize = '15')\n",
    "    plt.setp( ax.xaxis.get_majorticklabels(), rotation=70 )\n",
    "    \n",
    "    plt.savefig(plot_filename+'.png')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Bias Across Test Data: 67.441\n",
      "videographer: 64.302\n",
      "dietician: 66.251\n",
      "scientist: 69.774\n",
      "carpenter: 70.328\n",
      "accountant: 70.572\n",
      "lawyer: 71.999\n",
      "undertaker: 72.430\n",
      "photographer: 72.451\n",
      "writer: 74.968\n",
      "programmer: 74.990\n",
      "bookkeeper: 75.556\n",
      "plumber: 75.761\n",
      "receptionist: 80.458\n",
      "nurse: 80.694\n",
      "pharmacist: 81.293\n",
      "electrician: 81.495\n",
      "singer: 81.622\n",
      "translator: 81.885\n",
      "salesperson: 82.403\n",
      "judge: 83.610\n",
      "butcher: 84.738\n",
      "dental hygienist: 85.003\n",
      "fisherman: 85.243\n",
      "artist: 87.710\n",
      "pilot: 87.959\n",
      "nutritionist: 89.606\n",
      "attorney: 89.689\n",
      "surgeon: 90.612\n",
      "psychologist: 90.894\n",
      "bartender: 95.146\n",
      "physician's assistant: 95.746\n",
      "therapist: 96.320\n",
      "editor: 96.612\n",
      "painter: 98.702\n",
      "farmer: 99.898\n",
      "coach: 100.080\n",
      "designer: 101.173\n",
      "professor: 101.311\n",
      "jeweler: 101.810\n",
      "developer: 102.513\n",
      "economist: 103.225\n",
      "filmmaker: 104.165\n",
      "veterinarian: 104.898\n",
      "police officer: 106.440\n",
      "politician: 106.643\n",
      "flight attendant: 107.963\n",
      "barber: 108.055\n",
      "optician: 111.122\n",
      "businessperson: 114.574\n",
      "dentist: 114.806\n",
      "physician: 116.086\n",
      "teacher: 120.285\n",
      "doctor: 123.379\n",
      "engineer: 124.713\n",
      "builder: 129.790\n",
      "banker: 132.472\n",
      "architect: 132.487\n",
      "cashier: 136.640\n",
      "air traffic controller: 138.948\n",
      "secretary: 148.856\n",
      "chef: 150.779\n",
      "musician: 156.472\n",
      "scholar: 158.457\n",
      "mechanic: 161.005\n"
     ]
    }
   ],
   "source": [
    "#Biases for original data\n",
    "pr_bias = calculate_bias(\"../../data/sentence_biases.small.original-billion.v1.npy\")\n",
    "# plot_result(pr_bias,\"original_small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"original_small.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Perplexity Across Male Test Data: 434.282\n",
      "Mean Perplexity Across Female Test Data: 429.387\n",
      "Mean Bias Across Test Data: -4.894\n",
      "air traffic controller: -8.768\n",
      "dentist: -8.568\n",
      "architect: -8.429\n",
      "doctor: -8.384\n",
      "physician: -8.272\n",
      "optician: -8.208\n",
      "economist: -8.056\n",
      "mechanic: -8.022\n",
      "cashier: -7.906\n",
      "businessperson: -7.826\n",
      "veterinarian: -7.821\n",
      "chef: -7.791\n",
      "builder: -7.650\n",
      "scholar: -7.559\n",
      "physician's assistant: -7.518\n",
      "jeweler: -7.495\n",
      "musician: -7.394\n",
      "flight attendant: -7.390\n",
      "developer: -7.379\n",
      "accountant: -7.349\n",
      "dietician: -7.234\n",
      "butcher: -7.074\n",
      "secretary: -7.066\n",
      "coach: -6.979\n",
      "banker: -6.978\n",
      "designer: -6.974\n",
      "artist: -6.941\n",
      "pharmacist: -6.884\n",
      "judge: -6.859\n",
      "teacher: -6.794\n",
      "photographer: -6.741\n",
      "receptionist: -6.594\n",
      "undertaker: -6.544\n",
      "engineer: -6.533\n",
      "painter: -6.519\n",
      "nutritionist: -6.515\n",
      "bartender: -6.505\n",
      "editor: -6.398\n",
      "therapist: -6.355\n",
      "attorney: -6.345\n",
      "singer: -6.336\n",
      "translator: -6.210\n",
      "politician: -6.103\n",
      "barber: -6.091\n",
      "farmer: -6.052\n",
      "videographer: -6.042\n",
      "pilot: -6.012\n",
      "salesperson: -5.980\n",
      "police officer: -5.939\n",
      "electrician: -5.917\n",
      "bookkeeper: -5.824\n",
      "psychologist: -5.715\n",
      "carpenter: -5.675\n",
      "filmmaker: -5.630\n",
      "writer: -5.572\n",
      "plumber: -5.327\n",
      "programmer: -5.293\n",
      "fisherman: -5.291\n",
      "lawyer: -5.266\n",
      "nurse: -5.003\n",
      "scientist: -4.973\n",
      "surgeon: -4.656\n",
      "professor: -4.648\n",
      "dental hygienist: -4.580\n"
     ]
    }
   ],
   "source": [
    "#Biases for boosted data\n",
    "pr_bias = calculate_bias(\"../../data/sentence_biases.small.boosted-billion.v1.npy\",\\\n",
    "                         mp_filename = \"../../data/male_perplexities.small.boosted-billion.v1.npy\",\\\n",
    "                         fp_filename=\"../../data/female_perplexities.small.boosted-billion.v1.npy\")\n",
    "# plot_result(pr_bias,\"boosted_small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"boosted_small.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
